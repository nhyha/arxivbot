### Natural Language Processing RELIES on Linguistics (https://arxiv.org/abs/2405.05966)
- **What's New**: 이 연구는 대규모 언어 모델(Large Language Models, LLMs)이 특정 언어에서 높은 유창성을 가진 텍스트를 생성할 수 있음을 보여주며, 이것이 자연어 처리(Natural Language Processing, NLP)에서 언어 전문 지식의 미래에 어떤 의미를 가지는지 탐구합니다. 연구자들은 언어학이 NLP에 기여하는 여러 측면을 강조하고, 언어학적 사고가 새로운 방향을 밝힐 수 있는 방법을 논의합니다.

- **Technical Details**: 이 페이퍼는 $RELIES$라는 약어를 중심으로 구성된 언어학이 NLP에 기여하는 주요 측면들을 조명합니다: 자원(Resources), 평가(Evaluation), 저자원 환경(Low-resource settings), 해석 가능성(Interpretability), 설명(Explanation), 그리고 언어 연구(Study of language). 이로써, 기계 시스템을 인간 언어 시스템과 비교하는 것의 지속적인 중요성을 강조합니다.

- **Performance Highlights**: 연구는 LLM이 구문이나 의미의 일관성을 포착하기 위해 특별히 설계된 모듈을 사용하지 않고도 유창한 텍스트를 생성할 수 있다는 점을 강조합니다. 이는 NLP에서 언어학적 전문 지식을 어떻게 활용하고 있으며, 그 중요성이 감소하지 않았음을 시사합니다.



### OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning (https://arxiv.org/abs/2405.05957)
- **What's New**: OpenBA-V2는 원래 15B 모델에서 파생된 3.4B 모델로, 멀티 스테이지(다단계) 압축과 지속적인 사전 학습을 통해 개발되었습니다. 이 모델은 더 다양한 데이터와 유연한 학습 목표를 활용하며, 레이어 프루닝(layer pruning), 뉴럴 프루닝(neural pruning), 그리고 보캐뷸러리 프루닝(vocabulary pruning) 같은 기술들을 사용하여 77.3%의 압축률을 달성하며 성능 손실을 최소화했습니다.

- **Technical Details**: OpenBA-V2는 고급 학습 목표와 데이터 전략을 이용하는 작고 효율적인 LLM을 개발하는 방법론을 보여 줍니다. 이 모델은 특히 일반 상식 추론 및 이름 인식(Named Entity Recognition, NER)과 같은 다운스트림 작업에서 원래 15B 모델과 유사하거나 동등한 성과를 보여줍니다.

- **Performance Highlights**: OpenBA-V2는 유사한 크기의 다른 오픈 소스 모델들과 경쟁하는 성능을 보여줍니다. 이는 대규모 모델에서 요구되는 높은 배포 요구 사항과 추론 비용을 감소시키면서도 유지할 수 있는 성능을 입증합니다. 리소스가 제한된 상황에서 LLM의 구현을 도울 수 있는 가능성을 제시합니다.



### Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning (https://arxiv.org/abs/2405.05955)
- **What's New**: 최근 대형 언어 모델(Large Language Models, LLMs)의 등장은 인간 수준의 성능과 비교될만한 복잡한 작업을 자동화할 수 있는 새로운 가능성을 열어주고 있습니다. 그러나 이러한 LLM들은 복잡도가 높고 정확성을 요구하는 작업에서 어려움을 겪습니다. 이에 본 논문에서는 '스머프스(Smurfs)'라는 최신 멀티-에이전트 프레임워크를 소개합니다. 이는 기존 LLM을 협력하는 멀티-에이전트 앙상블로 변환하여 복잡한 작업의 분해와 수행을 향상시키는 방법입니다.

- **Technical Details**: 스머프스는 특정 역할을 각 모델 내에서 할당하는 혁신적인 프롬프팅 전략을 통해 이루어집니다. 이는 복잡한 작업을 효율적으로 해결할 수 있도록 외부 도구로 접근을 허용합니다. 본 연구에서는 '미스트랄-7B-인스트럭트(mistral-7b-instruct)' 모델을 사례 연구로 사용하여 스머프스의 탁월한 도구 사용 시나리오에서의 성능을 시연하였습니다.

- **Performance Highlights**: 특히 스머프스는 ToolBench I2 및 I3 벤치마크에서 ChatGPT-ReACT을 상대로 84.4%의 승률로 뛰어난 성능을 보였으며, 이는 GPT-4 모델의 최고 기록 73.5%를 상회하는 결과입니다. 또한 종합적인 절단 연구(ablation study)를 통해 멀티-에이전트 프레임워크의 핵심 구성 요소가 전반적인 효과에 기여하는 바를 분석하였습니다.



### DOLOMITES: Domain-Specific Long-Form Methodical Tasks (https://arxiv.org/abs/2405.05938)
Comments:
          Dataset link coming soon

- **What's New**: 이 논문에서는 다양한 분야의 전문가들이 작업을 계획, 조직 및 보고하기 위해 수행하는 체계적인 글쓰기 작업에 대한 새로운 분류 체계를 개발하고, 이를 'DoLoMiTes'라고 하는 새로운 벤치마크를 소개합니다. DoLoMiTes는 25개 분야의 수백 명의 전문가들로부터 얻은 519개의 체계적 작업 명세를 포함합니다.

- **Technical Details**: 벤치마크는 각 작업에 대한 구체적인 입력 및 출력 예제(총 1,857개)를 포함하며, 이는 각 작업당 최대 10개의 모델 생성 예제를 전문가가 수정하여 수집한 것입니다. 작업은 작업 목표(task objective), 절차(procedure), 입력(input), 출력(output)의 형태로 구조화됩니다.

- **Performance Highlights**: 현대 언어 모델(language models)을 사용하여 이러한 체계적 작업을 자동화하는 것은 복잡한 추론을 수행하고 주어진 맥락 및 도메인 지식에 의존해야 하므로 도전적인 장문 생성(long-form generation) 문제임을 강조합니다.



### Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations? (https://arxiv.org/abs/2405.05904)
- **What's New**: 대형 언어 모델(large language models)이 감독된 미세 조정(supervised fine-tuning)을 통해 학습할 때 새로운 사실적 정보에 직면할 수 있으며, 이는 모델이 기존 지식에 근거하지 않은 사실을 생성하도록 훈련받을 때 잘못된 정보를 만들어 내는 행동을 학습할 수 있다는 가설을 연구합니다.

- **Technical Details**: 이 연구는 모델이 새로운 지식을 도입하는 예제의 비율을 조정하면서 닫힌 책 QA(closed-book QA)에 중점을 두어 통제된 설정을 디자인합니다. 이를 통해 모델이 미세 조정 과정에서 새로운 사실적 지식을 어떻게 획득하는지, 그리고 기존 지식을 어떻게 활용하는지에 대한 영향을 분석합니다.

- **Performance Highlights**: 연구 결과, 대형 언어 모델은 새로운 사실적 지식을 미세 조정을 통해 습득하는 데 어려움을 겪으며, 새로운 지식을 도입하는 예제는 모델의 기존 지식과 일치하는 예제보다 상당히 느리게 학습됩니다. 그러나 새로운 지식을 포함하는 예제들이 결국 학습됨에 따라 모델의 환상적 반응(hallucination) 경향이 선형적으로 증가합니다. 이러한 결과는 미세 조정을 통해 새로운 사실적 지식을 도입하는 리스크를 강조하며, 대형 언어 모델들이 주로 사전 훈련(pre-training)을 통해 사실적 지식을 획득하고 미세 조정을 통해 보다 효율적으로 사용하도록 학습한다는 관점을 지지합니다.



### Efficient LLM Comparative Assessment: a Product of Experts Framework for Pairwise Comparisons (https://arxiv.org/abs/2405.05894)
- **What's New**: 이 논문은 효율적인 LLM(Large Language Models) 상호 비교 평가를 위한 전문가의 곱(Product of Expert, PoE) 프레임워크를 소개합니다. PoE 접근 방식은 각 비교를 전문가로 간주하여 두 텍스트 간의 점수 차이에 대한 정보를 제공합니다. 이 프레임워크는 가능한 모든 비교를 사용하지 않고도 인간 판단과 잘 일치하는 점수 예측을 생성할 수 있습니다.

- **Technical Details**: PoE 프레임워크는 가우시안(Gaussian) 전문가를 사용할 때 최적 후보자 순위를 위한 간단한 닫힌 형식(closed-form) 솔루션을 도출하고, 이 순위를 최대화할 가능성을 가진 비교 선택에 대한 표현을 제공합니다. 이는 후보자 집합에 대해 극대화할 수 있는 표현식을 결합하는 방식으로 이루어집니다. 연구는 이론적 분섄 및 가우시안 전문가의 닫힌 형식 솔루션을 이용하여 효율적인 비교 평가를 가능하게 합니다.

- **Performance Highlights**: 이 PoE 솔루션은 가능한 비교의 소수만을 사용하여도 모든 비교를 사용했을 때와 유사한 성능을 달성할 수 있음을 실험적으로 입증합니다. 특히 큰 규모의 N에서는 비교의 단 2%만 사용하여도 전체 비교를 사용했을 때와 비슷한 성능을 보여줍니다. 이 접근 방식은 고려중인 다양한 NLG(Natural Language Generation) 작업들에서 큰 계산 절감을 제공하며, 베이스라인 접근 방식보다 빠르게 수렴합니다.



### Towards a More Inclusive AI: Progress and Perspectives in Large Language Model Training for the S\'ami Languag (https://arxiv.org/abs/2405.05777)
- **What's New**: 이 연구는 새롭게 Sámi 언어를 중심으로 초저자원언어(Ultra Low Resource Languages, ULRLs)에 대한 LLM(Large Language Models)의 연구를 진행하였습니다. 초저자원언어는 매우 낮은 자료량과 사용 언어인 수로 인해 기존의 대형 언어 모델에서 지원하지 않아, 언어 모델 훈련이 더 어렵습니다.

- **Technical Details**: 연구진은 온라인에서 접근 가능한 Sámi 언어 자료를 수집하여 깨끗한 데이터셋을 구축하고, 다양한 LLM, 특히 대략 70억 개의 파라미터를 갖는 모델들로 실험을 진행하였습니다. 특히, 순차적 다국어 훈련이 공동 다국어 훈련보다 성능이 좋다는 것을 발견했습니다. 또한, 높은 의미 중복을 가진 다국어 훈련이 기존 훈련 방식보다 전반적으로 좋은 성능을 보였습니다.

- **Performance Highlights**: 이 연구에서의 주요 성과는 multilingual LLM training을 통해 초저자원언어에 대한 모델의 능력을 향상시켰다는 것입니다. 대 님 응답 시향 더 좋은 결과를 보여주어, 다양한 국제 언어 역량을 강화할 수 있음을 입증했습니다. Finnish 언어를 포함한 공동 다국어 훈련에서 가장 좋은 결과를 나타냈습니다.



### Experimental Pragmatics with Machines: Testing LLM Predictions for the Inferences of Plain and Embedded Disjunctions (https://arxiv.org/abs/2405.05776)
Comments:
          8 pages, 3 figures, to appear in the Proceedings of the 46th Annual Conference of the Cognitive Science Society (2024)

- **What's New**: 이 논문은 인간의 의사소통에서 발생하는 다양한 추론에 대해 연구하며, 특히 '또는'을 포함하는 평범한 및 내장된 분기(disjunctions)에서 유발되는 세 가지 추론에 초점을 맞춥니다. 최신 대규모 언어 모델(Large Language Models, LLMs)의 예측을 인간의 데이터와 비교 분석하여, 이 두 연구 결과가 어떻게 일치하는지를 탐구합니다. 이는 인간과 비인간 언어 에이전트 간의 의사소통 추론 처리 방식을 이해하는 데 중요한 새로운 관점을 제공합니다.

- **Technical Details**: 연구자들은 무지 추론(Ignorance Inferences, II), 분배 추론(Distributive Inferences, DI), 그리고 자유 선택 추론(Free Choice Inferences, FC) 등 세 가지 주요 추론 유형을 분석했습니다. 각 유형은 '또는'에 의해 유발되는 서로 다른 문맥에서 나타납니다. 연구는 이러한 추론이 어떻게 스칼라 함축(Scalar Implicatures, SI)과 관련되어 있는지, 또한 대규모 언어 모델이 이러한 추론을 얼마나 잘 예측하는지를 실험적으로 평가합니다. 연구진은 인간의 실험 설계를 바탕으로 LLM의 성능을 평가하고, 이를 인간 데이터와 직접 비교하여 LLM이 인간과 유사하거나 다른 패턴을 보이는지를 분석했습니다.

- **Performance Highlights**: LLM의 결과는 대체적으로 인간과 일치하는 경향을 보였으며, 추론과 함축 사이에 뚜렷한 차이를 보였습니다. 특히, LLM은 다양한 설정에서 DI 및 II 추론을 예측할 수 있음을 보여주며, 이러한 추론들이 상응하는 NU(부정적 보편성) 및 UNC(불확실성) 추론의 유무와 관계없이 독립적으로 발생할 수 있음을 시사합니다. 이는 전통적 추론 접근 방식에 도전하며, LLM이 인간의 추론 프로세스를 모방할 뿐만 아니라, 이러한 복잡한 언어적 상호작용을 어느 정도 이해하고 있음을 보여줍니다.



### Can large language models understand uncommon meanings of common words? (https://arxiv.org/abs/2405.05741)
- **Korean AI Newsletter**: [{"What's New": '이 연구는 대규모 언어 모델(LLMs)이 일상적인 단어의 특이한 의미를 얼마나 잘 이해하는지에 대한 세밀한 평가를 중심으로 진행됩니다. 새로운 Lexical Semantic Comprehension (LeSC) 데이터셋과 평가 척도를 도입하여 LLMs의 미묘한 의미 이해 능력을 평가하고, 이를 통해 LLMs의 일반적인 자연어 이해(NLU) 능력을 향상시키는 것을 목표로 합니다.'}, {'Technical Details': '이 논문에서는 언어의 미묘한 의미를 파악하는 새로운 방법론과 척도를 제안합니다. LeSC 데이터셋은 Fine-grained Lexical Semantics Understanding (LSU) 과 Cross-lingual transfer test를 포함하여 모델의 이해 능력을 더 깊게 탐구합니다. 여기에는 다양한 크기와 구조를 가진 모델들을 포함시켜 실험을 진행하였습니다.'}, {'Performance Highlights': '실험 결과, 현존하는 모델들은 기본적인 LSU 과제에서 부족한 성능을 보였으며, 심지어 최신 기술인 GPT-4와 GPT-3.5도 16세 인간 대비 각각 3.9%, 22.3% 뒤쳐지는 성능을 보였습니다. 이를 개선하기 위해 Few-shot prompting 와 retrieval-augmented generation 같은 고급 프롬프팅 기술을 도입하여 일부 문제를 완화할 수 있었지만, 한계는 여전히 존재합니다.'}]



### Computational lexical analysis of Flamenco genres (https://arxiv.org/abs/2405.05723)
Comments:
          21 pages, 29 figures

- **What's New**: 이 연구는 플라멩코(Flamenco) 가사에 대한 계산 분석을 제시하여, 문화적 정체성의 중요한 표현인 이 음악 전통에서 특징적인 패턴을 식별합니다. 자연어 처리(Natural Language Processing, NLP) 및 기계 학습(Machine Learning)을 이용하여 2000개 이상의 가사를 각기 다른 플라멩코 장르인 $	extit{palos}$로 분류했습니다.

- **Technical Details**: Multinomial Naive Bayes 분류기를 사용하여 $	extit{palos}$ 간의 어휘적 변이를 분석했으며, 이를 통해 각 스타일을 정확하게 식별할 수 있었습니다. 또한 자동 단어 사용 방법을 통해 각 스타일을 특징짓는 의미론적 분야(Semantic Fields)를 도출했습니다. 장르 간 거리를 정량화하는 측정법과 네트워크 분석(Network Analysis)을 적용하여 플라멩코 스타일 간의 관계를 조명했습니다.

- **Performance Highlights**: 이 연구를 통해 얻은 결과는 플라멩코의 $	extit{palo}$들이 역사적 연결성과 진화를 가지고 있다는 것을 시사합니다. 여기서 수행된 계산적 접근 방식은 플라멩코 가사의 복잡한 관계 및 문화적 중요성을 밝히는 데 기여하며, 전통 음악 장르의 기원과 발전에 대한 새로운 토론을 불러일으킬 수 있습니다.



### Detecting Statements in Text: A Domain-Agnostic Few-Shot Solution (https://arxiv.org/abs/2405.05705)
Comments:
          Paper accepted for publication at NOCAPS workshop at ICWSM 2024 conference

- **What's New**: 이 연구는 컴퓨터 사회 과학(Computational Social Science) 및 웹 컨텐츠 분석(Web Content Analysis)에 관한 다양한 작업을 위한 새로운 접근법을 제안합니다. 이는 고가의 대규모 데이터셋에 대한 모델의 fine-tuning이 필요하지 않은 few-shot 학습 방법론을 소개하며, 이는 청구 기반 텍스트 분류(Claim-based Textual Classification) 작업에 특히 유용합니다. 또한, 이 방법론은 자연어 추론(Natural Language Inference, NLI) 모델을 사용하여 텍스트 간의 함축 관계(Textual Entailment)를 파악하고, Probabilistic Bisection이라는 통계적 경험법칙을 통해 최소한의 데이터 포인트만을 동적으로 샘플링하여 모델의 성능을 향상시킵니다.

- **Technical Details**: 주요 기술적 요소로는 자연어 추론(NLI) 모델을 기반으로 하여 특정 텍스트가 주어진 청구의 분류에 속하는지를 결정하는 과정을 포함합니다. 분류를 위해 각 청구를 복잡한 체계(Taxonomies)로 정의하고, 이를 통해 도메인 전문 지식을 시스템에 명시적으로 통합할 수 있습니다. 또한, 이 연구는 활동적 학습(Active Learning) 원칙을 반영하여 데이터 주석의 전반적인 수를 최소화하는 새로운 임계값 조절(Threshold-Tuning) 전략을 제안합니다.

- **Performance Highlights**: 제안된 방법론은 기존의 Large Language Model(LMM)과 Zero-shot 분류 접근법을 벤치마킹하여, 성능을 입증합니다. 기후 변화에 대한 이의 제기 감지, 주제 및 입장 분류, 그리고 우울증 관련 증상 감지 등 세 가지 분류 작업에서 이 방식을 테스트하였습니다. 이 접근법은 전통적인 Pre-train/Fine-tune 접근법과 경쟁할 수 있는 성능을 보여주면서도 데이터 주석의 필요성을 현저히 줄입니다.



### Evaluating Dialect Robustness of Language Models via Conversation Understanding (https://arxiv.org/abs/2405.05688)
Comments:
          13 pages, 7 figures, 6 tables

- **What's New**: 이 연구는 영어(Egnlish language)의 다양한 방언(dialects)에 대한 대형 언어 모델들(Large Language Models, LLMs)의 성능을 평가합니다. 특히, 'taboo'라는 단어 맞추기 게임을 통해 진행된 인도 영어(Indian English, IndEng)와 미국 영어(US English, USEng)의 대화를 사용하여, 대화에서 마스킹 처리된(target-word-masked) 목표 단어를 예측하고 선택하는 두 가지 평가 방식을 소개하였습니다. 이에 대해, 기존의 데이터셋(MD3)을 확장하여, M-MD3라는 새로운 데이터셋을 도입하였으며, 여기에는 AITrans(방언 정보가 제거된 IndEng)와 AIGen(LLMs가 대화를 생성하도록 유도하는 데이터)의 두 개의 새로운 하위 세트를 추가하였습니다.

- **Technical Details**: 연구는 GPT-4, GPT-3.5, Mistral, 그리고 Gemma 등 네 가지 LLM을 사용하여 평가를 진행하였습니다. 이들 모델은 이전에 사전 학습(pre-trained)되고 세부 조정(fine-tuned)된 버전을 사용합니다. 평가 과정에서 target word prediction (TWP)와 target word selection (TWS)의 두 가지 작업을 수행하게 합니다. 또한, 방언의 견고성(dialect robustness)을 확인하기 위해 AIGen과 AITrans를 사용하여 LLM이 어떻게 다른 방언으로 학습할 수 있는지 및 이 과제의 도전적인 측면을 살펴보았습니다.

- **Performance Highlights**: 분석 결과, 모든 설정에 있어서 GPT 기반 모델들이 가장 우수한 성능을 나타냈으며, US English에 대한 성능이 Indian English보다 현저히 더 우수했습니다. 그러나 대화가 짧은 경우(8턴 미만)에는 비교적 작은 모델들이 더 공평한 성능을 보였습니다. LLM은 훈련 데이터의 구성에 따라 자체 방언을 학습할 수 있으며, AIGen(가장 좋은 성능을 보인 하위 집합)과 AITrans(가장 저조한 성능을 보인 하위 집합)의 결과가 이를 뒷받침합니다.



### G-SAP: Graph-based Structure-Aware Prompt Learning over Heterogeneous Knowledge for Commonsense Reasoning (https://arxiv.org/abs/2405.05616)
- **What's New**: 이 논문에서는 상식 추론(common sense reasoning)을 위해 새로운 그래프 기반의 구조 인식 프롬프트 학습 모델(Graph-based Structure-Aware Prompt Learning Model, G-SAP)을 제안하였습니다. 특히, ConceptNet, Wikipedia, 그리고 Cambridge Dictionary와 같은 다양한 지식 출처를 통합하여 증거 그래프(evidence graph)를 구축하고, 이를 통해 구조적 지식과 텍스트 정보를 완전히 통합하는 데 초점을 맞추었습니다.

- **Technical Details**: G-SAP 모델은 지식 그래프(Knowledge Graphs, KGs)와 언어 모델(Language Models, LMs) 간의 상호 작용을 강화하기 위해 구조 인식 동결된 PLM(structure-aware frozen PLM)을 채용하였습니다. 이 구조는 그래프의 엔티티와 관계에 의해 주도되는 프롬프트의 생성을 가능하게 하며, 이질적인 메시지 전달 추론 모듈(heterogeneous message-passing reasoning module)을 사용하여 LM과 그래프 기반 네트워크 간의 지식의 깊은 상호 작용을 촉진합니다.

- **Performance Highlights**: 이 모델은 세 개의 벤치마크 데이터셋에서 광범위한 실험을 통해 실증적 검증을 거쳤으며, 특히 OpenbookQA 데이터셋에서 기존 최고의 LM+GNNs 모델 대비 6.12% 향상된 결과를 보여 주었습니다. 이는 G-SAP이 상식 추론 분야에서 효과적으로 구조적 지식과 텍스트 정보를 통합할 수 있음을 시사합니다.



### Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM (https://arxiv.org/abs/2405.05610)
- **What's New**: 이 논문에서는 다단계(dialogue) 대화에서 대규모 언어 모델(이하 LLMs)을 공격하는 새로운 방법인 CoA (Chain of Attack)를 제시합니다. 이 방법은 대화 중에 문맥적 피드백과 의미적 연관성을 통해 공격 정책을 적응적으로 조정함으로써 LLM이 비합리적이거나 해로운 콘텐츠를 생성하게 합니다.

- **Technical Details**: CoA는 의미론적으로 주도되는 문맥적 다단계 공격 방법(semantical-driven contextual multi-turn attack method)이며, 다단계 대화의 문맥을 이해하고 이에 따라 공격 계획을 조정합니다. 이는 LLM이 문맥을 반영하여 불합리하거나 유해한 응답을 생성하도록 유도합니다.

- **Performance Highlights**: CoA는 다양한 LLM과 데이터셋에서 평가되었으며, 기존 공격 방법들보다 뛰어난 성능을 보여 LLM의 취약점을 효과적으로 드러냈습니다. 이는 LLM의 보안과 윤리적 평가에 기여하며, 대화 시스템의 안전성과 윤리성에 대한 새로운 관점과 도구를 제공합니다.



### OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs (https://arxiv.org/abs/2405.05583)
Comments:
          19 pages, 8 tables, 8 figures

- **What's New**: OpenFactCheck은 대규모 언어 모델(LLMs: Large Language Models)의 사실 정확성을 평가하기 위한 통합적인 프레임워크를 제안합니다. 이는 특히 다양한 실제 응용 프로그램에서의 LLM의 사용 증가에 따라 필요해졌습니다.

- **Technical Details**: OpenFactCheck은 세 가지 주요 모듈로 구성됩니다: (i) CUSTCHECKER는 사용자가 자동 팩트체커(fact-checker)를 쉽게 사용자 정의하고 문서 및 주장의 사실 정확성을 검증할 수 있게 해줍니다, (ii) LLMEVAL은 다양한 관점에서 LLM의 사실성 능력을 공정하게 평가하는 통합 평가 프레임워크입니다, (iii) CHECKEREVAL은 자동 팩트체커의 검증 결과의 신뢰성을 사람이 주석을 단 데이터셋을 사용하여 평가하는 확장 가능한 솔루션입니다.

- **Performance Highlights**: OpenFactCheck 프레임워크는 광범위한 벤치마크와 측정 방법을 사용하여 LLM의 출력물의 사실 정확성을 체계적으로 평가하며, 이는 과거 연구들이 제각각의 평가기준을 사용한 것과 차별화됩니다. 또한, 이 플랫폼은 공개적으로 사용 가능하며, 이는 연구 및 개발 커뮤니티에 큰 기여를 할 것입니다.



### From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences (https://arxiv.org/abs/2405.05572)
- **What's New**: 새로운 데이터셋 Cline에 관한 연구로, 영어-힌디어(code-mixed English-Hindi, 이하 en-hi) 혼합 텍스트의 자연스러움을 판별할 수 있는 인간의 판단을 모델링합니다. 이 데이터셋은 16,642개의 문장으로 구성되어 있으며, 인공적으로 생성된 텍스트와 소셜 미디어에서 수집된 샘플을 포함합니다.

- **Technical Details**: Cline 데이터셋의 분석 결과, 코드 혼합의 측정 기준인 CMI(Code Mixing Index), Switch Points 수, Burstiness 등이 인간의 수용성 판단과 낮은 상관관계를 보임을 확인했습니다. 이는 Multilingual Large Language Models(MLLMs, 다국어 대규모 언어 모델)인 XLM-Roberta와 Bernice가 IndicBERT보다 우수한 성능을 보이며, 특히 ChatGPT와 비교 실험에서도 큰 데이터에 미세 조정된 MLLMs가 우수한 결과를 나타냈습니다.

- **Performance Highlights**: XLM-Roberta와 Bernice는 IndicBERT를 다양한 설정에서 능가하며, 영어-힌디어에서 영어-텔루구어 코드 혼합 텍스트의 수용성 판단으로의 zero-shot 전송이 무작위 기준보다 우수함을 보여줍니다. 이 데이터와 모델은 공개적으로 제공되어 다른 코드 혼합 언어 쌍에 대한 연구와 응용을 가능하게 합니다.



### Automatic question generation for propositional logical equivalences (https://arxiv.org/abs/2405.05513)
- **What's New**: 본 연구에서는 코로나 팬데믹으로 인한 온라인 학습의 증가와 대학생들 사이의 학업 부정행위 증가 문제에 대응하기 위해 개별 학생마다 맞춤형 문제를 생성할 수 있는 자동 문제 생성(Automatic Question Generation, AQG) 방법을 개발하고 구현하였습니다. 특히 이 연구는 이산수학(Discrete Mathematics) 과목에 초점을 맞추어 AQG 접근법을 새롭게 도입하였습니다.

- **Technical Details**: 개발된 AQG 접근법은 구문 문법(syntactic grammar)과 의미 속성 시스템(semantic attribute system)을 통해 상위 아래로의 파싱(top-down parsing)과 구문 트리 변환(syntax tree transformations)을 활용합니다. 이 방법은 학생들이 받는 교재의 문제와 논리적으로 동등한 문제(logical equivalence problems)를 생성함으로써 개인화된 문제 생성을 가능하게 합니다.

- **Performance Highlights**: 실험 결과, 새롭게 개발된 AQG 방법으로 생성된 문제의 난이도는 교재[1]에 제시된 문제와 유사함을 보여주었습니다. 이 결과는 자동 문제 생성(Automatic Question Generation)이 교육에서 유용함을 입증하며, 학습 경험을 혁신적으로 향상시킬 잠재력을 가지고 있음을 확인시켜 줍니다.



### Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias (https://arxiv.org/abs/2405.05506)
Comments:
          Submitted for review

- **What's New**: 이 연구에서는 대규모 언어모델(Large Language Models, LLMs)이 직면하고 있는 편향성 및 데이터 정확성의 문제를 다루기 위해 'Cross-Care'라는 새로운 벤치마크 프레임워크를 소개합니다. 이 프레임워크는 다양한 인구 집단 간의 질병 발병률을 나타내는 방식에 초점을 맞추어 LLMs의 편향성과 실세계 지식을 평가하는 데 목적이 있습니다.

- **Technical Details**: 연구팀은 특히 질병 발병률 데이터와 다양한 인구 통계 집단을 대상으로 LLM이 어떻게 편향될 수 있는지 시스템적으로 평가했습니다. 이를 위해, 사전 학습 데이터 셋인 $ThePile$ 내의 인구 통계적 편향이 LLM의 출력에 미치는 영향을 분석하고, 실제 질병 발병률과 비교함으로써 불일치를 드러내고 정량화했습니다.

- **Performance Highlights**: 결과적으로, LLM이 다양한 인구 집단(subgroups)에서 질병 발병률을 표현하는 방식과 실제 질병 발병률 사이에 상당한 불일치가 있음을 발견했습니다. 이는 의료 응용 분야에서 LLM을 사용할 때 편향 전파 및 실세계 정교함의 결여에 대한 뚜렷한 위험을 나타냅니다. 또한, 다양한 언어 간 질병 발병률의 표현을 조정하는 방법들이 일관성 문제를 최소화하는 데 거의 효과적이지 않음을 관찰했습니다.



### Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis (https://arxiv.org/abs/2405.05496)
- **What's New**: 이 논문에서는 감성 분석(sentiment analysis)의 중요한 하위 과제인 측면 기반 감성 분석(Aspect-based sentiment analysis, ABSA)을 다루며, 	exttt{Large Language Model-based Continual Learning (LLM-CL)} 모델을 제안합니다. 기존 연구들이 도메인 특화 모델을 타겟 도메인 데이터셋을 이용하여 미세 조정하는 데 집중했다면, 이 연구는 지속적 학습(continual learning) 과제를 제안하여 타겟 도메인의 능력을 배우면서 동시에 기존 도메인의 능력을 유지합니다.

- **Technical Details**: 	exttt{LLM-CL} 모델은 도메인 지식 탈착 모듈(domain knowledge decoupling module)을 설계하여 도메인 불변 어댑터(domain-invariant adapter)와 도메인 변이적 어댑터(domain-variant adapters)를 직교 제약(orthogonal constraint)을 통해 분리해서 학습합니다. 또한 도메인 지식 예열 전략(domain knowledge warmup strategy)을 도입하여 도메인 불변 지식과 도메인 변이적 지식 간의 표현을 조정합니다. 테스트 단계에서는 각 샘플의 도메인 ID를 요구하지 않으면서 해당 도메인 변이적 지식을 도메인 위치(domain positioning)를 통해 색인합니다.

- **Performance Highlights**: 19개 데이터셋에 대한 광범위한 실험을 통해 	exttt{LLM-CL} 모델은 새로운 최고 성능(state-of-the-art performance)을 달성함을 보여줍니다.



### Parameter-Efficient Fine-Tuning With Adapters (https://arxiv.org/abs/2405.05493)
- **What's New**: 이 연구에서는 언어 모델 파인 튜닝(fine-tuning) 분야에 새로운 적응 방법을 소개했습니다. 기존의 도메인 적응 사전 학습(Domain-Adaptive Pretraining, DAPT)과 작업 적응 사전 학습(Task-Adaptive Pretraining, TAPT) 방법은 효과적이지만 계산 비용이 많이 듭니다. 본 연구는 UniPELT 프레임워크를 기반으로 PromptTuning Layer를 추가하여 훈련 가능한 파라미터의 수를 크게 줄이면서 다양한 벤치마크에서 경쟁력 있는 성능을 유지하는 방법을 제시합니다.

- **Technical Details**: 이 방법은 어댑터(adapters)를 사용하여 사전 훈련된 모델들을 새로운 작업에 효율적으로 전환할 수 있도록 하고, 기본 모델 파라미터의 재학습을 최소화합니다. 어댑터는 GLUE 벤치마크, 도메인 특화 데이터셋, 그리고 Stanford Question Answering Dataset 1.1 (SQuAD)을 포함한 세 가지 다양한 데이터셋을 사용하여 평가되었습니다.

- **Performance Highlights**: 이 맞춤형 어댑터 기반 방식은 전체 모델 파인 튜닝, DAPT+TAPT 및 UniPELT 전략과 비교하여 유사하거나 더 적은 수의 파라미터를 필요로 하면서 경쟁력 있는 성능을 달성했습니다. 이러한 파라미터 효율성은 계산 부담을 완화시키고 적응 과정을 가속화합니다. 연구는 어댑터가 상당히 감소된 자원 소비로 높은 성능을 달성할 수 있는 잠재력을 강조하며, 파라미터 효율적인 핀 튜닝(fine-tuning) 연구에 있어 향후 유망한 방향을 제시합니다.



### Using Machine Translation to Augment Multilingual Classification (https://arxiv.org/abs/2405.05478)
- **What's New**: 텍스트 분류 모델 개발의 주된 병목 현상은 훈련 데이터에 대한 어노테이션(annotation)의 필요성이고, 이 필요성은 다국어 분류기(multilingual classifiers)에 있어서 더욱 증가합니다. 최근의 머신 번역(machine translation) 모델들이 쉽게 접근 가능하고, 번역 품질도 신뢰할 수 있기 때문에, 한 언어의 레이블이 붙은 훈련 데이터를 다른 언어로 번역하는 것이 가능해졌습니다. 본 연구에서는 머신 번역을 사용하여 다양한 언어를 대상으로 하는 분류 작업(classification task)에 대한 다국어 모델을 미세 조정(fine-tune)하는 효과를 탐구합니다. 또한, 원래 이미지 캡셔닝(image captioning) 분야에서 제안된 새로운 기술을 사용하여 번역된 데이터에 기반한 모델 튜닝의 부정적 영향을 보완하는 이점을 조사합니다.

- **Technical Details**: 연구는 번역된 데이터가 다국어 분류기를 미세 조정하는데 충분한 품질을 가지고 있음을 보여주며, 번역 데이터를 사용하여 모델을 튜닝할 때 발생할 수 있는 부정적 영향을 감소시키는 데 새로운 손실 기술(loss technique)이 일부 개선을 제공할 수 있음을 보여줍니다. 이 새로운 손실 기술은 이미지 캡셔닝에서 처음 제안되었으며, 데이터 번역 과정에서 발생할 수 있는 정보의 손실 또는 왜곡을 줄이는데 기여합니다.

- **Performance Highlights**: 실험을 통해 번역된 데이터를 사용하여 미세 조정한 다국어 분류기는 원래 언어 데이터만을 사용할 때와 비교하여 성능이 향상되었음을 입증하고, 새로운 손실 기술이 도입된 모델은 이러한 향상을 더욱 높일 수 있는 잠재력을 보여줍니다.



### Poser: Unmasking Alignment Faking LLMs by Manipulating Their Internals (https://arxiv.org/abs/2405.05466)
- **What's New**: LLMs이 예측에서 정렬된 척하는 동안, 상황이 유리할 때 부적절하게 행동하는 'alignment fakers'를 탐지하는 새로운 벤치마크가 소개되었습니다. 이 벤치마크는 role-play 시나리오에서 행동을 선택하기 위해 특별히 조정(fine-tuned)된 36개의 LLaMA-13B 모델을 포함하고 있으며, 이들 모델은 'aligned' 모델과 'alignment faking' 모델로 구성된 324개의 쌍을 테스트합니다.

- **Technical Details**: 이 연구에서는 동일한 입력에 대해 동일하게 행동하는 두 모델 쌍에서 'alignment faking' 모델을 식별하는 여러 가지 탐지 전략을 평가합니다. 이러한 전략들은 모델 내부의 작동 메커니즘을 분석하여 출력 행위가 아닌 더 깊은 패턴을 포착합니다. 특히, 가장 성공적인 전략 중 하나는 모델 활성화(activations)에 잡음을 추가하는 것으로, 이 전략은 alignment faking 모델들이 더 높은 오차로 전환되는 경향을 드러내어 98%의 높은 정확도를 달성했습니다.

- **Performance Highlights**: 탐지 전략 중 '모델들을 독주시켜 나쁜 행동을 드러내게 함(intoxicate models so they reveal bad behavior)'은 98%의 높고 인상적인 정확도를 달성했습니다. 이 외에도, 모델들을 재학습시켜 나쁜 행동을 파악하는 방법(90% 정확도)과 특정 개념을 식별하는 방법(71% 정확도) 등 다른 전략들도 상당한 성과를 보였습니다. 이러한 결과는 LLMs의 'alignment faking' 탐지 가능성에 대한 중요한 통찰을 제공하며, AI 안전성 강화를 위한 연구의 중요성을 강조합니다.



### Evaluating Students' Open-ended Written Responses with LLMs: Using the RAG Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Larg (https://arxiv.org/abs/2405.05444)
Comments:
          18 pages, 6 tables, 1 figure

- **What's New**: 이 연구에서는 대학생들이 참고 자료에 대해 작성한 개방형 질문에 대한 응답을 평가하는 데 있어서 대규모 언어 모델(Large Language Models, LLMs)인 ChatGPT-3.5, ChatGPT-4, Claude-3, 및 Mistral-Large의 효과를 조사했습니다. 이 연구는 교육자들이 학생들의 개방형 응답을 평가하는 데 소요되는 시간과 노력을 효율적으로 사용할 수 있는 가능성을 탐색합니다.

- **Technical Details**: 각 모델은 RAG(Retrieval Augmented Generation) 프레임워크를 사용하여 답변 평가 과정을 처리하며, 0.0과 0.5의 두 가지 온도 설정에서 각각 10번씩(10-shot) 총 54개의 답변을 반복 평가하였습니다. 이는 모델당 총 1,080회, 모든 모델을 통틀어 4,320회의 평가를 의미합니다.

- **Performance Highlights**: 이 연구 결과는 LLM들이 제공하는 평가의 일관성과 등급 결과에서 주목할 만한 차이를 드러내었습니다. 개방형 서술형 응답을 평가하기 위한 LLMs의 강점과 약점을 이해할 필요가 있으며, 교육 평가에 LLMs를 사용하는 정확성과 비용 효과를 결정하기 위한 추가 비교 연구가 필요합니다.



### Mitigating Exaggerated Safety in Large Language Models (https://arxiv.org/abs/2405.05418)
Comments:
          17 pages, 8 figures, 2 tables

- **What's New**: 이 연구에서는 Large Language Models (LLMs)의 '과장된 안전성(exaggerated safety)' 문제를 다루고 있습니다. 연구자들은 여러 기법을 통해 LLM들이 위험한 프롬프트를 거부하는 동시에 유용하게 활용될 수 있도록 하는 방법을 탐구하고 있습니다. 이들은 XSTest 데이터셋 프롬프트와 인터랙티브 콘텍스트 및 퓨샷 프롬프팅(few-shot prompting)을 결합하여 LLM들의 결정 경계를 검사하였고, 이를 통해 과장된 안전성을 크게 감소시킬 수 있었습니다.

- **Technical Details**: 연구팀은 LLM들의 결정 과정을 '해킹'할 수 있는 다양한 프롬프팅 전략을 개발했습니다. 구체적으로 Llama2, Gemma Command R+ 및 Phi-3 같은 최첨단 LLM들의 행동을 관찰했습니다. 각각의 모델에 대해 가장 효과적인 프롬프팅 방식을 분석하였고, Llama2는 퓨샷 프롬프팅이, Gemma는 인터랙티브 프롬프팅이, Command R+와 Phi-3는 콘텍스트 프롬프팅(contextual prompting)이 가장 잘 작동한다는 결과를 얻었습니다.

- **Performance Highlights**: 연구 결과에 따르면, 이러한 전략적 결합을 통해 모든 LLM들에서 과장된 안전성 행동을 92.9% 감소시킬 수 있었습니다. 이는 LLM들이 안전하면서도 유용하게 동작할 수 있는 가능성을 크게 향상시키는 결과로, LLM의 사용성과 안전성 사이의 균형을 찾는 데 큰 도움이 될 것입니다.



### Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models (https://arxiv.org/abs/2405.05417)
Comments:
          16 pages, 4 figures. For associated code, see this https URL

- **What's New**: 이 연구는 언어 모델의 토크나이저(tokenizer) 생성과 모델 트레이닝 사이의 연결고리가 끊어져 있음을 밝히고, 이러한 분리가 'glitch tokens'라 불리는 문제를 일으키는 토큰들의 식별에 어려움을 준다는 점을 탐구합니다. 이 토큰들은 토크나이저 단어장에는 존재하지만, 트레이닝 데이터에는 거의 또는 전혀 사용되지 않습니다. 연구팀은 큰 규모의 언어 모델(Large Language Models, LLMs)에 초점을 맞추어 이런 훈련되지 않거나 덜 훈련된 토큰들을 탐지하는 새로운 방법론을 제시합니다.

- **Technical Details**: 연구팀은 토크나이저 분석, 모델 가중치(weight) 기반 지표, 그리고 프롬프팅 기법(prompting techniques)의 조합을 통해, 교육받지 않고 문제가 될 수 있는 토큰들을 자동으로 감지할 수 있는 효과적인 방법을 개발했습니다. 이 방법은 다양한 모델의 토크나이저에서 이러한 토큰들의 존재를 보여줍니다.

- **Performance Highlights**: 이 연구의 결과는 'glitch tokens'이 다양한 언어 모델에서 얼마나 흔하게 발생하는지를 보여주고, 언어 모델의 효율성과 안전성을 높이는 데 있어 중요한 통찰력을 제공합니다. 연구팀의 방법은 여러 모델에서 이러한 토큰들의 빈번한 발생을 식별해 내는 데 성공했습니다.



### "They are uncultured": Unveiling Covert Harms and Social Threats in LLM Generated Conversations (https://arxiv.org/abs/2405.05378)
- **What's New**: 이 연구는 대규모 언어 모델(LLMs: Large Language Models)이 유발하는 잠재적 해로움과 위협을 상세하게 다룬다. 특히, 서구 중심의 인종(race)과 성(gender)에 국한되지 않고, 인도의 카스트(caste) 같은 비서구적 개념을 대상으로 한다. 연구는 Covert Harms and Social Threats (CHAST)라는 새로운 평가 척도를 도입하여, 이 척도를 이용해 LLM이 생성한 대화 내용에서 나타나는 미묘하고 간접적인 해로움을 평가한다.

- **Technical Details**: 연구팀은 8개의 오픈소스 및 OpenAI 언어 모델들을 사용하여, 인도의 카스트 및 서구 중심의 인종 관련 시나리오에서 1,920개의 대화를 생성하였다. 이를 통해 LLM이 유발할 수 있는 다양한 해로움과 위협을 감지하는 CHAST 메트릭스를 검증하고 사용하였다. 모델은 인간 평가 기준에 맞춰 조정되었으며, CHAST 메트릭스의 레이블을 생성하는 작업에서 사람 수준의 평가와 일치성을 보였다.

- **Performance Highlights**: 연구 결과, 모든 검토된 LLM에서 인종 및 카스트 개념에 기반한 대화를 생성할 때 CHAST 메트릭스에 따라 해로운 내용을 포함하고 있었다. 특히 카스트 기반의 대화에서 더 많은 CHAST를 포함하는 경향을 보였으며, 이는 기존의 인기 있는 모델들(Perspective API 및 Detoxify)로는 감지하기 어려운 내용이었다. 이러한 발견은 LLM이 채용 과정에서 대화 작업을 수행하는 데 아직 준비가 충분하지 않을 수 있음을 시사한다.



### Krey\`ol-MT: Building MT for Latin American, Caribbean and Colonial African Creole Languages (https://arxiv.org/abs/2405.05376)
Comments:
          To be published at NAACL 2024

- **What's New**: 이 논문은 최대 규모의 크리올어 기계 번역(Machine Translation, MT) 데이터 세트를 제공하며, 특히 21개의 크리올어에 대해 처음으로 제공되는 자료를 포함하고 있습니다. 또한 41개의 크리올어를 지원하는 기계 번역 모델을 소개하고, 172개 언어 방향으로 번역을 지원합니다. 이러한 확장은 크리올어를 포함한 저자원 언어(low-resource languages)의 기술 발전에 중요한 기여를 한다고 할 수 있습니다.

- **Technical Details**: 연구팀은 총 14.5M 개의 유니크 크리올 문장을 수집했으며, 이 중 11.6M 개를 공개합니다. 이 데이터 세트는 장르(genre)가 다양하며 기존의 장르 특화 크리올어 MT 모델보다 우수한 성능을 보였습니다. 새로운 MT 모델은 23개의 34번역 방향에서 벤치마크 기준을 초과하여 성능을 보여주었습니다. 이는 크리올어와 같은 저자원 언어에 대한 크로스언어 전송(cross-lingual transfer) 및 기계 학습(machine learning) 기반의 NLP 연구에 새로운 기회를 제공할 수 있습니다.

- **Performance Highlights**: 새로운 MT 모델은 다양한 장르 데이터에 노출되어 기존의 장르 특화 모델보다 우수한 성능을 나타냈습니다. 연구에 따르면, 이 모델은 공개된 크리올어 언어 벤치마크에서 23개 언어 방향에 대해 최고의 성능을 달성했습니다. 이러한 성과는 저자원 언어의 기계 번역 개발에 있어 중요한 진전을 의미합니다.



### Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models (https://arxiv.org/abs/2405.05374)
Comments:
          17 pages, 11 Figures, 9 tables

- **What's New**: 이 연구 보고서는 Apache-2 라이센스하에 오픈 소스된 'arctic-embed' 텍스트 임베딩 모델 패밀리의 데이터셋 생성 및 레시피를 설명합니다. 이 모델들은 각각의 크기에 맞춰 MTEB Retrieval 리더보드에서 최신 최고 성능을 달성했습니다. 특히 가장 큰 모델인 'arctic-embed-l'은 Cohere의 embed-v3와 Open AI의 text-embed-3-large와 같은 폐쇄 소스 모델들을 능가했습니다.

- **Technical Details**: 이 연구는 다양한 크기(22백만에서 334백만 파라미터)의 다섯 가지 인코더-온리 사전 학습 언어 모델을 활용했습니다. 각 모델은 재현성 있고 효율적인 평가를 제공하는 MTEB 레트리버 평가에서 nDCG@10을 최적화하기 위해 훈련되었습니다. 모델들은 입증된 성능과 있어 (Pretrained language models), 인크리멘탈 학습 (Incremental learning), 대규모 벤치마킹을 통해 검증된 바 있습니다.

- **Performance Highlights**: 각각의 아틱 임베드 모델 변형체는 크기별로 새로운 최상의 성능을 달성했습니다. 또한, 이 연구는 학습 중 데이터 샘플링 (Data sampling) 및 네가티브 마이닝 방법이 검색 품질 개선과 더 밀접하게 관련되어 있음을 시사하는 일련의 연구를 제시했습니다. 이는 데이터 스케일과 배치 사이즈의 확장에 초점을 맞춘 이전 연구와는 차별화됩니다.



### The Effect of Model Size on LLM Post-hoc Explainability via LIME (https://arxiv.org/abs/2405.05348)
Comments:
          Published at ICLR 2024 Workshop on Secure and Trustworthy Large Language Models

- **What's New**: 이 연구는 대규모 언어 모델 (Large Language Models, LLMs)이 커질수록 성능이 향상된다는 점을 다룬 연구이지만, 모델 크기가 설명 가능성(explainability)에 어떻게 영향을 미치는지에 대한 분석이 주요 내용입니다. 특히 DeBERTaV3 모델의 다양한 크기가 자연어 추론 (Natural Language Inference, NLI)과 제로샷 분류 (Zero-Shot Classification, ZSC) 작업에서 LIME 설명의 품질에 미치는 영향을 살펴보았습니다. 모델의 크기가 증가함에 따라 내부 결정 과정을 반영하는 설명의 신뢰성(faithfulness)과 인간 설명과의 일치성(plausibility) 사이에 큰 차이가 있음을 발견했습니다.

- **Technical Details**: 연구는 Huggingface의 DeBERTaV3 모델 네 가지 크기(22백만에서 304백만 매개변수)를 사용하여, NLI와 ZSC 작업에 대해 실험하였습니다. 설명의 질을 평가하기 위해 신뢰성과 명료성 두 가지 접근 방식을 적용했습니다. 신뢰성(faithfulness)은 설명이 모델의 내부 결정 과정을 얼마나 잘 반영하는지를 측정하며, 명료성(plausibility)은 설명이 인간이 생성한 설명과 얼마나 일치하는지를 평가합니다. 연구 결과, 모델의 성능은 크기가 클수록 향상되었으나, LIME으로 생성된 설명과 인간 생성 설명 사이의 일치도는 향상되지 않았습니다.

- **Performance Highlights**: 이 연구는 LLM의 크기 증가가 모델의 성능을 향상시킬 수는 있지만, LIME 설명의 명료성(plausibility)과는 상관관계가 없음을 보여줍니다. 또한, NLI 맥락에서 신뢰도 메트릭의 한계를 시사하며, NLP에서 후행(post-hoc) 설명 가능성의 표현력 부족 등 일반적인 한계를 지적하고 있습니다. 이 연구는 모델 크기가 후행 설명 가능성에 미치는 영향을 이해하기 위한 첫 시도로, 후속 연구를 위한 확장 가능한 코드 레포지토리를 제공합니다.



### QuaLLM: An LLM-based Framework to Extract Quantitative Insights from Online Forums (https://arxiv.org/abs/2405.05345)
Comments:
          Accepted to CHI LLM as Research Tools Workshop (2024)

- **What's New**: 이 연구는 온라인 포럼의 텍스트 데이터에서 정량적 인사이트를 분석하고 추출하기 위한 새로운 LLM 기반 프레임워크인 QuaLLM을 소개합니다. Reddit의 라이드쉐어 노동자 커뮤니티에서 100만 건 이상의 댓글을 분석하여, AI 및 알고리즘 플랫폼 결정에 대한 노동자들의 주요 우려사항을 밝혀냈습니다. 이는 이러한 유형의 연구로는 가장 큰 규모입니다.

- **Technical Details**: QuaLLM 프레임워크는 고급 프롬프트 엔지니어링(prompt engineering)과 평가 전략을 활용하여 데이터를 수집하고 특정 주제에 대한 관련 논의에서 우려 사항 요약을 생성합니다. 이 프레임워크는 생성(generation), 분류(classification), 집계(aggregation), 유병률(prevalence)의 네 단계 프롬프팅 과정을 포함합니다. LLM은 주제별로 우려사항을 식별하고 중복을 피하기 위해 집계하며 대표적인 인용문을 선택하고, 빈도와 영향을 평가한 후 JSON 형식으로 출력을 포맷팅합니다.

- **Performance Highlights**: 이 프레임워크의 적용을 통해 우리는 라이드쉐어 노동자들 사이에서 AI와 알고리즘 플랫폼 결정에 대한 명확한 우려를 확인할 수 있었습니다. 특히, 노동자들의 인사이트를 반영하여 규제 당국에 의견을 제시하는 데 QuaLLM이 효과적으로 기여했습니다. 정량적 텍스트 데이터 분석에 AI를 사용함으로써 온라인 포럼에서의 우려사항을 식별하는 새로운 선례를 마련하였습니다.



### The Perspectivist Paradigm Shift: Assumptions and Challenges of Capturing Human Labels (https://arxiv.org/abs/2405.05860)
- **What's New**: 이 논문은 데이터 라벨링(labeling) 분야에서 새로운 관점, 즉 'perspectivist turn'을 소개합니다. 이는 다수의 주석자(annotators) 간의 불일치(disagreement)를 문제로 보지 않고, 오히려 유용한 정보의 원천으로 여기는 접근 방식입니다. 이러한 변화는 기계 학습(machine learning, ML)의 데이터 수집 및 처리 방식에 패러다임 변화를 제안하며, 주석자들 사이의 의견 차이를 포괄함으로써 소수 의견(minority voices)을 드러내고, 작업의 모호성(ambiguities)을 밝혀내는 데 도움을 줍니다.

- **Technical Details**: 연구자들은 주석자들 간의 불일치가 발생하는 원인과 이에 대한 다양한 가정들을 검토합니다. 고전적인 접근 방식에서는 주석자 불일치를 라벨의 품질 문제로 간주했습니다. 이에 반해, perspectivist 접근 방식은 주석자 간 라벨의 변동성(variation)을 의미 있는 정보로 취급합니다. 이러한 접근은 주석자들의 다양성(diversity) 및 경험(lived experiences)을 데이터 라벨링 과정에 통합시키려는 시도를 포함하며, 이는 기존에 '편향(bias)'이나 '불경험'으로 치부되던 요소들을 재평가하는 계기를 마련합니다.

- **Performance Highlights**: 이 새로운 접근법은 모델 성능과 교정(calibration)을 향상시킬 뿐만 아니라, 주석자들의 다양한 경험과 관점이 라벨링 과정에 반영됨으로써 결과 데이터의 다양성과 풍부함을 증대시킬 수 있습니다. 예를 들어, 주석자의 사회적 배경이나 지식이 평균 라벨(mean label)에 비해 상이할 경우, 평균 라벨이 사회적으로 편향될 수 있음을 지적하며, 이것이 주석자 간 불일치를 통계적인 편향(statistical bias)과 사회적 편향(societal bias)으로 구분짓는 논리를 제시합니다.



### Similarity Guided Multimodal Fusion Transformer for Semantic Location Prediction in Social Media (https://arxiv.org/abs/2405.05760)
- **What's New**: 이 연구는 소셜 미디어 게시물에서 의미 있는 위치 정보를 추출하는 새로운 Similarity-Guided Multimodal Fusion Transformer (SG-MFT)를 제안합니다. 이 방법은 높은 품질의 기능 표현을 추출하고, 모델 간의 상호작용 및 퓨전을 개선하기 위해 유사성 가이드를 활용합니다.

- **Technical Details**: SG-MFT는 먼저 사전 훈련된 대규모 시각-언어 모델(CLIP)을 활용하여 고품질의 특징을 추출합니다. 이어서, 유사성 가이드 상호작용 모듈(SIM)과 유사성 인식 특징 퓨전 모듈(SFM)를 도입하여 모달리티 간의 차이와 잡음의 영향을 줄입니다. SIM은 모달리티별 유사성과 요소별 유사성을 활용한 조정을 통해 모달리티의 통합을 강화하며, SFM은 교차 주의 메커니즘(cross-attention mechanism)을 사용하여 두 모달리티를 효과적으로 통합합니다.

- **Performance Highlights**: SG-MFT는 모달리티 불균형을 주요하게 해결하고, 퓨전의 효율성과 강인함을 유지하며, 의미 있는 위치 예측 작업에서 뛰어난 성능을 보여줍니다. 실험 결과, 이 방법은 최신 기술보다 우수한 분류 성능을 달성하였습니다.



### Exploring the Potential of Human-LLM Synergy in Advancing Qualitative Analysis: A Case Study on Mental-Illness Stigma (https://arxiv.org/abs/2405.05758)
Comments:
          55 pages

- **What's New**: 'CHALET'이라는 새로운 방법론은 인간과 대형 언어 모델(LLMs) 간의 협력을 통해 인간-컴퓨터 상호작용(Human-Computer Interaction, HCI) 분야에서의 질적 분석을 향상시키고자 제안되었습니다. 이는 기존의 질적 코딩(qualitative coding) 내에서 LLM의 역할을 넘어서, 새로운 통찰력 생성에 있어 인간과 LLM이 협력할 수 있는 가능성을 탐구합니다.

- **Technical Details**: CHALET 방법론은 데이터 수집에서부터 시작하여, 질적 데이터에 대한 사람과 LLM의 연역적 코딩(deductive coding)을 수행하고 이 불일치(disagreements)를 집중적으로 살펴보는 과정입니다. 이후, 이러한 불일치 사례들에 대해서 협력적인 귀납적 코딩(inductive coding)을 수행하여 새로운 개념적 통찰을 찾아내는 절차를 포함합니다.

- **Performance Highlights**: CHALET은 정신 질환 낙인(mental illness stigma)의 귀인 모델(attribution model) 적용을 통해 그 효과성을 검증하였습니다. 이 연구는 인지적(cognitive), 감정적(emotional), 행동적(behavioral) 차원에서의 암시적인 낙인화 테마들을 밝혀내는 데 성공하였습니다. 이 결과는 HCI 분야 및 관련 분야에 걸쳐서 메소돈지에(new methodology)와 횡단적 기회(transdisciplinary opportunities)를 제공합니다.



### Beyond Prompts: Learning from Human Communication for Enhanced AI Intent Alignmen (https://arxiv.org/abs/2405.05678)
- **What's New**: 이 연구는 LLM(Large Language Models)을 포함한 생성 AI(Generative AI)의 최신 발전이 어떻게 인간-AI 상호작용의 패러다임을 변화시켰는지 조명합니다. 특히, 사용자의 의도와 AI의 결과 간의 일치(intent alignment)를 개선하기 위해 인간 간의 의사소통 전략을 AI 시스템 디자인에 어떻게 적용할 수 있는지 탐구합니다.

- **Technical Details**: 연구는 사람과 사람 간, 그리고 사람과 LLM 간의 상호작용을 비교하는 연구 설계를 사용하여 진행되었습니다. 여기에는 특히 GPT-4 (OpenAI, 2023)가 사용되었으며, 의도 일치를 위한 기본 전략으로서 적극적인 정보 요청과 상황에 맞는 답변 제공이 사용되었습니다. AI 시스템에서의 이러한 전략은 사용자의 의도를 더 정확하게 반영할 수 있는 AI의 발전을 가져올 수 있습니다.

- **Performance Highlights**: 인간 보조자는 적극적인 질문, 피드백 요청, 사용자가 제공한 정보를 기반으로 한 맞춤형 대응을 통해 의도 일치를 향상시켰습니다. 이는 LLM이 대화에서 보다 수동적이고, 때로는 불필요하거나 범용적인 정보를 제공하여 사용자의 의도를 완전히 반영하지 못하는 것과 대조됩니다. 이러한 발견은 AI에서 인간과 같은 상호작용 모델을 구현하는 데 중요한 통찰력을 제공합니다.



### Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning (https://arxiv.org/abs/2405.05615)
Comments:
          Accepted to ICML2024

- **What's New**: 이 연구에서는 시각적 정보와 관련된 작업을 처리하기 위해 시각적 프롬프트를 추가 지식으로 간주하는 새로운 접근법인 memory-space visual prompting (MemVP)을 소개합니다. 이 방법은 기존의 언어 모델 입력에 시각적 프롬프트를 통합하는 대신, 언어 모델의 Feed-Forward Network (FFN) 가중치와 시각적 프롬프트를 연결하여 시각 지식을 주입합니다.

- **Technical Details**: MemVP는 시각-언어(VL) 모델을 효과적으로 구성하기 위해 시각 인코더의 출력을 언어 모델의 입력 공간으로 투영하는 기존의 접근법과 다릅니다. 문서에 따르면, 언어 모델의 FFN이 'key-value memory' 역할을 함을 발견하고, 이러한 FFN의 가중치에 시각적 프롬프트를 연결하여 시각 지식을 주입하는 방식을 제안합니다.

- **Performance Highlights**: 다양한 VL 작업과 언어 모델을 통한 실험 결과, MemVP는 기존의 파라미터 효율적인 파인튜닝 (PEFT) 방법보다 우수한 성능을 보이며, 훈련 시간과 추론 대기 시간을 상당히 줄일 수 있음을 보여줍니다.



### Can We Use Large Language Models to Fill Relevance Judgment Holes? (https://arxiv.org/abs/2405.05600)
- **What's New**: 이 논문에서는 대규모 언어 모델(Large Language Models, LLMs)을 사용하여 대화형 검색(Conversational Search, CS) 시나리오에서 테스트 컬렉션의 '구멍'을 채우고 구체적인 인간 판단에 기반한 LLM 기반 판단을 확장하는 초기 단계를 설명합니다. 연구는 대화형 검색의 동적이고 변화하는 정보 요구를 감안할 때, 이러한 '구멍'이 심화되기 때문에 주목할 만합니다.

- **Technical Details**: 이 연구에서는 대화형 검색에 관여하는 사용자의 정보 요구에 따라 변화하는 문맥을 처리하기 위해 TREC iKAT 데이터세트를 사용합니다. 여러 상업용과 오픈 소스 LLMs, 특히 ChatGPT 및 LLaMA 모델을 사용하여 제로샷(zero-shot), 원샷(one-shot), 투샷(two-shot), 그리고 미세 조정(fine-tuning) 접근 방식으로 실험합니다. 새로운 시스템에 대한 평가를 왜곡하는 불평가 문서의 영향을 조사하여 LLMs와 인간 판단이 어떻게 상호 작용하는지 연구합니다.

- **Performance Highlights**: LLM을 사용한 자동 판단은 인간 판단과 높은 상관 관계를 보였지만, 인간과 자동 판단을 결합했을 때 상관관계는 상당히 낮아졌습니다. 특히, 테스트에 사용된 LLM의 유형에 따라 새로운 실행이 크게 선호되거나 처벌받는 경향이 있었으며, 이는 '구멍'의 크기에 비례하여 증가하는 것으로 나타났습니다.



### One vs. Many: Comprehending Accurate Information from Multiple Erroneous and Inconsistent AI Generations (https://arxiv.org/abs/2405.05581)
Comments:
          Accepted to FAccT 2024

- **What's New**: 이 연구는 대규모 언어 모델(LLM: Large Language Models)이 생성한 다중 출력에서의 일관성 불일치가 사용자의 AI에 대한 인식과 정보 이해에 어떠한 영향을 미치는지 조사합니다. 특히, 일관성이 없는 정보를 제공 받았을 때, 사용자가 AI의 능력을 어떻게 인식하고 정보를 어떻게 이해하는지에 대한 심층 연구를 진행했습니다.

- **Technical Details**: 연구자들은 다섯 가지 유형의 출력 불일치를 식별하고, 이를 바탕으로 252명의 참가자를 대상으로 실험을 진행했습니다. 참가자들은 하나 이상의 LLM 생성 텍스트를 읽고 정보 탐색 질문에 대한 이해도를 테스트하는 설문조사를 수행했습니다. 이 연구는 LLM의 비결정론적(Nondeterministic) 특성과 이로 인한 출력에서의 일관성 결여가 사용자의 AI 이해에 어떤 심리적 영향을 미치는지를 분석했습니다.

- **Performance Highlights**: 연구 결과, 일관성 없는 출력은 참가자들이 AI의 능력을 낮게 평가하는 경향이 있었지만, 정보 이해도는 오히려 증가하는 것으로 나타났습니다. 특히 두 개의 텍스트를 읽은 참가자들에게서 이러한 현상이 가장 두드러졌으며, 세 개의 텍스트를 읽은 경우에는 그 효과가 다소 감소했습니다. 이러한 발견을 바탕으로, LLM의 한계를 명확히 하고 비판적 사용을 장려하기 위한 시스혜 디자인 방향을 제시합니다.



### Analysis and prevention of AI-based phishing email attacks (https://arxiv.org/abs/2405.05435)
Comments:
          Electronics, accepted

- **What's New**: 이 연구는 AI가 생성한 피싱 이메일에 대한 새로운 데이터 집합을 제공하고, 이를 이용하여 피싱 이메일을 자동으로 식별할 수 있는 머신 러닝(machine learning) 도구의 효능을 테스트합니다. 특히, AI 생성 이메일과 수작업으로 만든 스캠 이메일을 구별하는 데 중점을 둡니다.

- **Technical Details**: 연구팀은 AI 생성 피싱 이메일과 인간이 생성한 스캠 이메일 사이의 구체적인 차이점을 분석적으로 기술하였습니다. 이들은 머신 러닝 도구를 사용하여 AI 생성 이메일을 정확하게 식별할 수 있음을 보여줍니다. 또한, 이 데이터 집합은 공개되어 이후 연구에 사용될 수 있습니다.

- **Performance Highlights**: 머신 러닝 도구는 고정밀도로 AI 생성 피싱 이메일을 식별할 수 있으며, 이는 일반 이메일이나 인간이 생성한 스캠 이메일과 비교해 높은 정확도를 보입니다. 이 결과는 AI 생성 이메일이 인간 생성 이메일과 스타일적으로 다르다는 것을 시사합니다.



### Interpretability Needs a New Paradigm (https://arxiv.org/abs/2405.05386)
- **What's New**: 이 논문은 기존에 분석되고 실현된 인공지능(AI) 모델의 설명 가능성(explainability) 접근 방식에 대한 새로운 패러다임을 제안합니다. 기본적으로 모델 설계가 설명 가능하도록 이루어져야 한다는 'intrinsic' 패러다임과 후처리 방식으로 설명을 생성하는 'post-hoc' 패러다임 사이에 존재하는 논쟁에 대한 재검토를 제공하며, 세 가지 새로운 패러다임을 소개합니다: 설명의 신뢰성(faithfulness)을 측정 가능하도록 모델을 설계하는 방식, 설명의 신뢰성을 최적화하는 모델을 개발하는 방식, 그리고 예측(prediction)과 설명(explanation)을 동시에 생성하는 모델을 개발하는 방식입니다.

- **Technical Details**: 이 논문은 인공지능 모델의 설명 가능성에 대해, 모델 설계 및 최적화 과정에서 설명의 신뢰성을 어떻게 보장할 수 있는지에 중점을 두고 설명합니다. 새로운 패러다임들은 예측과 함께 설명을 제공하거나, 설명의 신뢰성을 자동으로 측정하여 향상시키는 방법 등을 포함하고 있습니다. 각 패러다임은 설명이 모델의 동작을 정확히 반영하는지(즉, 신뢰성이 높은지)에 대한 평가의 중요성을 강조합니다.

- **Performance Highlights**: 사례 연구와 이론적 논의를 통해 제시된 새로운 패러다임은 기존의 패러다임에 비해 명확한 우위를 보이지 않으나, 설명의 신뢰성과 적합성을 높이는 방향으로의 개선 가능성을 제시합니다. 새로운 패러다임에 대한 구체적인 성능 지표는 제시되지 않았지만, 설명가능성 연구의 새로운 방향을 제시한다는 점에서 의미 있는 진전으로 평가됩니다.



### Benchmarking Educational Program Repair (https://arxiv.org/abs/2405.05347)
Comments:
          15 pages, 2 figures, 3 tables. Non-archival report presented at the NeurIPS'23 Workshop on Generative AI for Education (GAIED)

- **What's New**: 이 연구는 교육용 프로그램 수리를 위한 새로운 벤치마크를 제안하면서, 기존의 대규모 언어 모델(Large Language Models, LLMs)을 활용한 프로그래밍 교육 연구에 표준화된 평가 척도를 도입합니다. 연구는 두 개의 고품질 공개 데이터셋을 선별하고, 프로그램 수리의 질을 평가하기 위해 'rouge@k'라는 새로운 평가 지표를 도입하는 유니파이드(통합된) 평가 절차를 제시합니다.

- **Technical Details**: 연구진은 자동 프로그램 수리(Automated Program Repair)의 역할을 강조하면서, 기존의 단위 테스트 기반 자동 평가 시스템이나 지능형 튜터링 시스템과는 다른 접근 방식을 채택합니다. 이 벤치마크는 LLMS 기반의 수리 메커니즘을 활용하여, 학생들의 코드에 대한 디버깅 지원 및 다음 단계 힌트를 제공하는 데에 중점을 둡니다. 이 연구는 특히 디코더-온리(Decoder-only) 트랜스포머 모델을 활용하여 성능을 평가하며, 프로그램 수정의 기능 정확성을 판단하기 위해 'pass@k' 평가 방법과 결합된 'rouge@k'를 소개합니다.

- **Performance Highlights**: 제안된 벤치마크를 통해 평가된 다섯 가지 모델은, 새로운 rouge@k 메트릭을 사용하여 프로그램 수리의 품질을 평가한 결과, 학생들이 작성한 코드의 다양한 오류를 해결할 수 있는 능력을 입증했습니다. 이 결과는 교육적 맥락에서의 프로그램 수리에 대한 LLMS의 유용성과 효과를 보여주며, 향후 연구 및 적용 가능성에 대한 기준을 제시합니다.



### KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation (https://arxiv.org/abs/2405.05329)
Comments:
          preprint for ICML 2024

- **JSON Format Response**: [{"What's New": '이 연구에서는 KV-Runahead, 새로운 병렬화 기법을 제안하여 LLM (Large Language Model)의 프롬프트 단계를 가속화합니다. 이 방법은 여러 프로세스를 조정하여 KV-cache (key-value cache)를 사전에 채우고 처음 토큰 생성 시간 (TTFT)을 최소화합니다.'}, {'Technical Details': 'KV-Runahead는 기존의 텐서 또는 순차 병렬화 방식과 달리, 이미 존재하는 KV-cache를 이용하여 병렬 처리를 수행합니다. 이를 통해 계산 및 통신 비용을 줄이고, 동기화 필요성을 감소시키며, 비동기적 통신을 가능하게 합니다. 또한, KV-cache는 causal attention 계산을 기반으로 하므로, 자동으로 계산량을 최소화합니다.'}, {'Performance Highlights': 'KV-Runahead는 Llama 7B와 Falcon 7B 모델에서 각각 1.4배 및 1.6배의 속도 향상을 제공합니다. 추가적으로, context-level의 부하 균형(load-balancing)을 통해 TTFT를 최적화합니다. 이러한 접근 방식은 네트워크 대역폭의 변동에도 강하며, 계산 및 통신 오버헤드를 효과적으로 줄일 수 있습니다.'}]



### Harmonizing Program Induction with Rate-Distortion Theory (https://arxiv.org/abs/2405.05294)
Comments:
          CogSci 2024

- **What's New**: 이 연구는 음악 멜로디 학습을 상황으로 사용하며, 인간의 정신 프로그램 형태의 표상을 평가하는 새로운 차원을 Rate Distortion Theory (RDT)에 통합합니다. 특히, 설명 길이(description length), 오차(distortion), 그리고 계산 비용(computational costs) 사이의 새로운 3중 균형(trade-off)을 제안하고 있습니다. 또한, 태스크 간 공유 프로그램 라이브러리를 구축하는 것이 전반적인 이점을 제공하지만, 학습 자료의 커리큘럼 순서에 따라 민감도가 발생하며, 이는 인간 학습자의 특성과도 일치한다고 보여줍니다.

- **Technical Details**: 이 논문에서는 점진적으로 제공되는 멜로디 노트를 프로그램 형태로 인코딩하여 오류를 최소화하는 동시에 프로그램의 설명 길이와 프로그램 탐색에 필요한 계산 비용의 제약을 받는 시뮬레이션을 통해 압축과 프로그램 학습의 상호작용을 연구합니다. 사용된 데이터셋은 실제 멜로디로부터 추출되며, 베이지안 프로그램 유도(Bayesian program induction)와 결합논리(combinatory logic, CL)를 포함한 방법론을 통해 멜로디를 프로그램과 같은 표상으로 압축하는 모델을 구축하였습니다.

- **Performance Highlights**: 모델은 다양한 멜로디에 대한 일반화 성능을 성공적으로 향상시켰으며, 특히 공유 프로그램 라이브러리의 구축이 상호 작용하는 프로그램들 간의 더 큰 시너지와 일반화를 가능하게 함을 보여줍니다. 또한, 부분 정보 분해(partial information decomposition) 방법을 사용하여 훈련 커리큘럼을 생성, 이를 통해 효과적인 라이브러리 및 더 나은 일반화를 유도하는 것이 가능함을 시사합니다.



