New uploads on arXiv(cs.CL)

### Does Refusal Training in LLMs Generalize to the Past Tense? (http://arxiv.org/abs/2407.11969v1) [pdf: http://arxiv.org/pdf/2407.11969v1]
Code and jailbreak artifacts:
  https://github.com/tml-epfl/llm-past-tense

- **Summary**: The paper investigates the susceptibility of state-of-the-art large language models (LLMs) to refusal training bypass when harmful requests are reformulated in the past tense. The authors demonstrate that simply modifying harmful queries to the past tense significantly increases the success of bypassing the refusal mechanisms in models like Llama-3 8B, GPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o, and R2D2. For instance, the evasion rate for GPT-4o jumps from 1% to 88% with 20 past tense reformulation attempts compared to direct harmful requests. Interestingly, future tense reformulations are less effective, reflecting models' tendency to treat past events as less harmful. Fine-tuning experiments show that including past tense examples in the training data can bolster the models' resistance against these attacks. The paper highlights that existing alignment techniques like SFT, RLHF, and adversarial training do not always generalize effectively, suggesting a need for more robust training approaches.

- **PhD-Level Questions**: [{'Question 1': 'Discuss the implications of the finding that past tense reformulations significantly increase the success rate of harmful query bypass compared to present and future tense reformulations. How does this affect the design and deployment of safe LLMs?', 'Answer 1': "The finding implies that the alignment techniques currently in place can be easily circumvented by simple linguistic transformations, undermining the reliability of these models in critical applications. This necessitates a thorough revision of model training methods, including the incorporation of a broader range of linguistic contexts (such as tense variations) into refusal training datasets. Ensuring a model's ability to generalize refusal mechanisms across all possible reformulations is crucial for maintaining its safety and preventing harmful outputs."}, {'Question 2': "Based on the fine-tuning experiments with GPT-3.5 Turbo, explain how inclusion of past tense reformulation examples in training data might affect the model's alignment robustness. What are potential limitations of this approach?", 'Answer 2': "Including past tense examples during fine-tuning enhances the model's ability to recognize and refuse harmful queries regardless of their tense, thereby improving its alignment robustness. However, this approach might have limitations such as increased computational and data storage requirements, and it may not address other possible reformulation strategies (e.g., rephrasing, using synonyms). Additionally, the model might become overly conservative, refusing benign requests misclassified as harmful due to their past tense formulation."}, {'Question 3': 'Analyze how the success rate of past tense reformulations as an attack vector reflects on the brittleness of commonly used alignment techniques like SFT, RLHF, and adversarial training. What improvements can be proposed?', 'Answer 3': 'The high success rate of past tense reformulations suggests that these alignment techniques do not effectively cover different linguistic variations and fail to enforce comprehensive refusal behavior. To address this brittleness, improvements could include more diverse and comprehensive adversarial training datasets, enhanced linguistic pattern recognition, and adaptive learning mechanisms that continuously update and harden the refusal policies against novel reformulation tactics. Additionally, implementing multi-step verification processes involving context understanding and cross-reference with ethical guidelines might improve robustness.'}]



### NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window? (http://arxiv.org/abs/2407.11963v1) [pdf: http://arxiv.org/pdf/2407.11963v1]
- **Summary**: The paper introduces NeedleBench, a framework designed to evaluate the long-context capabilities of large language models (LLMs) through a series of progressively difficult tasks. These tasks take into account various text lengths (from 4,000 to over 1,000,000 tokens) and different depths of information placement to rigorously test the models' abilities to retrieve and reason with key information in bilingual contexts. The authors also propose the Ancestral Trace Challenge (ATC) to simulate real-world logical reasoning complexities, further assessing the LLMs' performance in practical long-context applications. The findings reveal that current LLMs require significant improvement in handling such tasks effectively. The resources for NeedleBench and the ATC are available on the OpenCompass GitHub repository.

- **PhD-Level Questions**: [{'Question 1': "Describe the rationale behind the design of NeedleBench and how it addresses the evaluation of long-context capabilities in LLMs. How does NeedleBench's approach differ from traditional performance benchmarks?"}, {'Question 2': 'Explain the Ancestral Trace Challenge (ATC) introduced in the paper. What are the specific logical reasoning challenges it aims to mimic, and how does this contribute to the assessment of LLMs in practical long-context tasks?'}]



### Rethinking Transformer-based Multi-document Summarization: An Empirical Investigation (http://arxiv.org/abs/2407.11948v1) [pdf: http://arxiv.org/pdf/2407.11948v1]
- **Summary**: The paper investigates the performance and behaviors of Transformer-based models in multi-document summarization (MDS). The study covers five empirical aspects: (1) the quantitative impact of document boundary separators; (2) the effectiveness of different mainstream Transformer structures; (3) the sensitivity of the encoder and decoder; (4) various training strategies; and (5) the presence and effects of repetition in summary generation. The experiments conducted on popular MDS datasets using eleven evaluation metrics reveal that document boundary separators significantly influence MDS outcomes, different levels of feature granularity affect performance, and training strategies play a crucial role. It was found that the decoder is more sensitive to noise than the encoder, emphasizing its critical role in summary quality. Additionally, the problem of repetition in generated summaries is linked to high uncertainty scores, pointing to areas for further research and improvement in MDS systems.

- **PhD-Level Questions**: [{'Question 1': 'Discuss the impact of document boundary separators on the performance of Transformer-based MDS models and explain why this impact is significant.'}, {'Question 2': 'Analyze why the decoder in a Transformer-based MDS model is more sensitive to noise than the encoder. What implications does this have for future research in MDS?'}, {'Question 3': 'Compare and contrast the effectiveness of various Transformer structures in the context of MDS as explored in the paper. How do these structures influence the summarization quality?'}, {'Question 4': 'Examine the role of different training strategies in Transformer-based MDS models. How do these strategies affect the final summary output?'}, {'Question 5': 'Explore the correlation between repetition in generated summaries and high uncertainty scores. What theories or mechanisms can explain this phenomenon? How might this be mitigated in future MDS models?'}]



### Fine-grained Hallucination Detection and Mitigation in Long-form Question Answering (http://arxiv.org/abs/2407.11930v1) [pdf: http://arxiv.org/pdf/2407.11930v1]
Code and data are available:
  https://github.com/UKPLab/arxiv2024-lfqa-hallucination

- **Summary**: Long-form question answering (LFQA) is designed to deliver thorough and detailed responses to complex inquiries, which is crucial for deep comprehension. However, LFQA responses often suffer from hallucinations and factual inaccuracies, complicating their evaluation. The authors introduce HaluQuestQA, the first dataset featuring localized error annotations for both human-written and model-generated LFQA answers. HaluQuestQA includes 698 QA pairs with 4700 span-level error annotations across five error types. Expert annotators also provide preference judgments. The researchers use this data to analyze LFQA shortcomings, finding that answers generally lack comprehensiveness and useful references. They trained an automatic feedback model to predict error spans and provide explanations for the errors. Additionally, they propose an 'Error-informed refinement' prompt-based approach that uses feedback from the model to improve generated answers, reducing hallucinations and enhancing quality. Human evaluators significantly prefer answers refined by this approach, with an 84% preference rate over baseline answers.

- **PhD-Level Questions**: [{'Question 1': 'Describe the methodology used in HaluQuestQA for annotating errors in LFQA answers. What are the five types of errors identified, and how do they contribute to the overall evaluation of answer quality?'}, {'Question 2': 'Explain the process and significance of training the automatic feedback model in the context of HaluQuestQA. How does this model contribute to error detection and the subsequent enhancement of answer quality in LFQA systems?'}, {'Question 3': "Discuss the 'Error-informed refinement' approach proposed in the paper. How does it utilize the signals from the feedback model to refine answers, and what are the implications of this method on the reduction of hallucinations in LFQA?"}, {'Question 4': 'Evaluate the experimental setup and data collection process for HaluQuestQA. How does the annotated dataset enable a comprehensive analysis of LFQA shortcomings, and what improvements were observed with the proposed methodologies?'}, {'Question 5': "Critically analyze the preference judgment aspect of the study. How do human evaluators' preferences provide insights into the effectiveness of the proposed approach, and what are potential limitations of relying on human judgments in this context?"}]



### What's Wrong? Refining Meeting Summaries with LLM Feedback (http://arxiv.org/abs/2407.11919v1) [pdf: http://arxiv.org/pdf/2407.11919v1]
- **Summary**: The paper discusses a novel approach to meeting summarization, leveraging Large Language Models (LLMs) for improved coherence and context understanding. Despite the potential of LLMs, they often struggle with maintaining relevance and avoiding hallucination. To address these issues, the authors propose a multi-LLM correction framework that mimics the human review process, involving two phases: mistake identification and summary refinement. They introduce the QMSum Mistake dataset, which contains 200 automatically generated meeting summaries annotated by humans for nine types of errors such as structural, omission, and irrelevance. Their experiments indicate that LLMs can accurately identify these mistakes, which can then be transformed into actionable feedback to enhance summary quality across various metrics. The post-hoc refinement leverages multiple LLMs to validate and improve output quality. The approach shows potential for complex text generation tasks needing robustness, action planning, and goal-oriented discussion.

- **PhD-Level Questions**: [{'Question 1': 'Explain the significance of a multi-LLM correction framework in enhancing the quality of meeting summaries. What are its primary phases, and why are they essential?'}, {'Question 2': 'Discuss the error types defined in the QMSum Mistake dataset and elaborate on how these errors impact the overall quality of automatically generated summaries.'}, {'Question 3': 'How does the proposed system convert identified errors into actionable feedback? Provide an example demonstrating this conversion process in the context of meeting summarization.'}, {'Question 4': 'Evaluate the potential benefits and limitations of leveraging multiple LLMs in text generation tasks. In what ways can this approach contribute to robustness and goal-oriented discussions?'}, {'Question 5': 'Consider the metrics used to evaluate summary quality (relevance, informativeness, conciseness, and coherence). Why is each metric important, and how might they interact or conflict in the context of meeting summarization?'}]



### A Novel Lexicon for the Moral Foundation of Liberty (http://arxiv.org/abs/2407.11862v1) [pdf: http://arxiv.org/pdf/2407.11862v1]
- **Summary**: This paper addresses the challenge of identifying and categorizing expressions related to the concept of liberty in various texts, particularly those concerning controversial social issues like vaccine hesitancy, climate change, and abortion rights. The authors propose a new 'Liberty lexicon,' which has been evaluated on over 3,000 manually annotated data points across different contexts. The final lexicon is a combination of several generated lexicons which utilize word embedding similarity and compositional semantics. Key contributions include developing a robust liberty lexicon, enriching the annotations of liberty-related expressions, and highlighting the complexity of such expressions across different platforms. The results indicate that the task is complex and would benefit from approaches that combine multiple types of knowledge to improve learning systems' representations of such concepts.

- **PhD-Level Questions**: [{'Question 1': 'How do the authors use word embedding similarity (WE) and compositional semantics (CS) to generate and improve the liberty lexicon?'}, {'Question 2': 'What are the challenges associated with annotating data for liberty-related expressions, and how does the proposed lexicon address these challenges?'}, {'Question 3': 'Explain the importance of evaluating the liberty lexicon in both in-domain and out-of-domain scenarios. How do these evaluations contribute to the overall robustness of the lexicon?'}, {'Question 4': 'Discuss the significance of combining multiple lexicons to create a final ensemble lexicon. What advantages does this approach provide over using a single lexicon?'}]



### Evaluating Task-Oriented Dialogue Consistency through Constraint Satisfaction (http://arxiv.org/abs/2407.11857v1) [pdf: http://arxiv.org/pdf/2407.11857v1]
- **Summary**: The paper addresses the issue of maintaining consistency in task-oriented dialogues, both within the dialogue itself and with external domain knowledge, by framing it as a Constraint Satisfaction Problem (CSP). In this framework, dialogue segments referencing the conversational domain are treated as variables, and the logical, linguistic, and domain-related consistency requirements are treated as constraints. A CSP solver is used to detect inconsistencies in dialogues that have been re-lexicalized by a Large Language Model (LLM). Results show that while CSP is an effective tool for detecting inconsistencies, maintaining consistent dialogue re-lexicalization is challenging for current state-of-the-art LLMs, with a reported accuracy of only 0.15 when compared to the CSP solver. An ablation study highlights that domain knowledge constraints are the hardest to uphold. The authors argue that CSP is a promising model for capturing essential dialogue consistency properties that have been inadequately addressed by traditional component-based approaches.

- **PhD-Level Questions**: [{'Question 1': 'Explain how framing dialogue consistency as a Constraint Satisfaction Problem (CSP) can enhance the detection of inconsistencies in task-oriented dialogues. What are the main components of this CSP framework?'}, {'Question 2': 'Discuss the challenges state-of-the-art LLMs face in achieving consistent dialogue re-lexicalization, as highlighted by the study. Why might domain knowledge constraints be particularly difficult for LLMs to satisfy?'}, {'Question 3': 'Based on the ablation study conducted in the paper, evaluate the impact of different types of constraints (linguistic, conversational, domain-based) on maintaining dialogue consistency. How might this influence the development of future dialogue systems?'}, {'Question 4': 'Compare and contrast CSP-based approaches to maintaining dialogue consistency with traditional component pipeline approaches. In what ways does CSP address the limitations of these traditional methods?'}, {'Question 5': 'The study reports a 0.15 accuracy rate for LLMs in maintaining dialogue consistency relative to the CSP solver. What methodological improvements could be proposed to increase this accuracy? How might these improvements impact the overall performance of dialogue systems?'}]



### Scaling Sign Language Translation (http://arxiv.org/abs/2407.11855v1) [pdf: http://arxiv.org/pdf/2407.11855v1]
- **Summary**: The paper addresses the challenge of Sign Language Translation (SLT) from video-based sign languages to text-based spoken languages. Current SLT research is often restricted to specific domains and a limited number of sign languages, making open-domain tasks difficult. This study advances SLT by increasing the pretraining data, model scale, and translation directions. The authors perform large-scale SLT pretraining using multilingual noisy YouTube data, parallel text corpora, and SLT data enhanced by translating video captions using machine translation models. They unify pretraining tasks with prompts tailored to the encoder-decoder architecture and initialize their SLT models with pretrained T5 models of various sizes. Results on datasets like How2Sign and FLEURS-ASL#0 show the benefits of scaling data/models and cross-modal, cross-lingual transfer, including zero-shot SLT capabilities. Further fine-tuning on five open-domain sign language benchmarks reveals significant quality improvements, surpassing previous state-of-the-art performances by a considerable margin.

- **PhD-Level Questions**: [{'Question 1': 'Explain the implications of scaling dataset size and model size in the context of Sign Language Translation (SLT). How did the authors demonstrate the impact of these scalings in their experiments?', 'Answer 1': 'Scaling the dataset size and model size is crucial for improving SLT as it allows the model to learn from a larger and more diverse set of examples, thereby improving its generalization capabilities. The authors demonstrate this by conducting large-scale pretraining on noisy multilingual YouTube SLT data, parallel text corpora, and augmented SLT data. They show that models initialized with pretrained (m/By)T5 models across various sizes yield substantial performance improvements on datasets like How2Sign and FLEURS-ASL#0 compared to smaller, less diverse training sets. The results highlight significant quality improvements and demonstrate the feasibility of zero-shot SLT, indicating that larger datasets and more complex models enable better cross-lingual and cross-modal transfer.'}, {'Question 2': 'What is the role of task-specific prompts in the unified pretraining approach employed by the authors, and how do they facilitate encoder-decoder architecture in SLT?', 'Answer 2': 'Task-specific prompts in the unified pretraining approach serve to guide the model in focusing on relevant aspects of the task at hand, enabling better specialization within a single, cohesive framework. In the context of the encoder-decoder architecture, these prompts help the model to properly encode the input video sequences and decode them into the corresponding textual outputs by defining clear task boundaries and expectations. By using such prompts, the authors improve the model’s ability to handle diverse pretraining tasks, ultimately enhancing its performance across various SLT benchmarks.'}]



### Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection (http://arxiv.org/abs/2407.11854v1) [pdf: http://arxiv.org/pdf/2407.11854v1]
Submitted to EMNLP 2024

- **Summary**: The paper addresses Grammatical Error Detection (GED) in low-resource languages, where human-annotated error corpora are not available. The authors leverage zero-shot cross-lingual transfer capabilities of multilingual pre-trained language models to generate synthetic errors in target languages. A two-stage fine-tuning pipeline is proposed: first, the GED model is fine-tuned on multilingual synthetic data from target languages; then, it is further fine-tuned on human-annotated GED corpora from source languages. This method outperforms current annotation-free GED techniques. Analysis shows that errors generated by this approach are more diverse and closer to human errors than those from other methods.

- **PhD-Level Questions**: [{'Question 1': 'Explain the two-stage fine-tuning pipeline proposed in the paper for training a GED model in low-resource languages. Why is this method considered to outperform current state-of-the-art annotation-free GED methods?'}, {'Question 2': 'Discuss the significance of zero-shot cross-lingual transfer capabilities in multilingual pre-trained language models. How do these capabilities contribute to generating synthetic errors in target languages?'}, {'Question 3': 'Analyze the benefits and potential limitations of using synthetic error corpora for GED training in low-resource languages. How does this approach ensure the diversity and human-like nature of the errors produced?'}, {'Question 4': "Compare and contrast the errors produced by the proposed method with those of other strong baselines. What does the increased diversity and similarity to human errors imply about the model's performance and reliability?"}]



### InferAct: Inferring Safe Actions for LLM-Based Agents Through Preemptive Evaluation and Human Feedback (http://arxiv.org/abs/2407.11843v1) [pdf: http://arxiv.org/pdf/2407.11843v1]
- **Summary**: The paper discusses the development of InferAct, an innovative approach designed to enhance the robustness of Large Language Model (LLM)-based agents. It leverages the Theory-of-Mind capability of LLMs to proactively identify potential mistakes before they result in critical actions, such as making irreversible purchases in online trading. InferAct also incorporates human feedback to mitigate risks and refine the decision-making processes of the LLM agents. The effectiveness of InferAct is demonstrated through experiments across three commonly used tasks, indicating its potential to improve safety and reliability in applications requiring high-stakes decisions.

- **PhD-Level Questions**: [{'Question 1': 'Discuss the significance of Theory-of-Mind capabilities in LLM-based agents and how InferAct utilizes this concept for preemptive error detection in decision-making tasks.'}, {'Question 2': 'Evaluate the potential implications of integrating human feedback with LLM-based agents as done in InferAct. How does this integration contribute to reducing irreversible risks in critical applications?'}, {'Question 3': 'Design an experiment to test the scalability of InferAct in new environments. What factors would you consider and why would they be important?'}, {'Question 4': 'Critically analyze the limitations of InferAct based on the information provided. What aspects of LLM-based decision-making does InferAct fail to address, and how might these limitations be mitigated in future research?'}, {'Question 5': 'Compare and contrast InferAct with other existing preemptive evaluation methods for LLM agents. What innovative aspects does InferAct introduce, and what challenges might it face in real-world application?'}]



New uploads on arXiv(cs.IR)

### Harnessing Large Language Models for Multimodal Product Bundling (http://arxiv.org/abs/2407.11712v2) [pdf: http://arxiv.org/pdf/2407.11712v2]
under review

- **Summary**: Product bundling is the process of offering a strategic combination of individual items to clients, and it has become crucial for online services. Existing methods for product bundling, despite their use of multimodal information extractors, face limitations like poor semantic understanding, limited knowledge scope, and cold-start issues. Large Language Models (LLMs) possess extensive knowledge and complex reasoning capabilities but traditionally struggle with processing multimodalities directly for product bundling tasks. The introduction of Bundle-LLM aims to address this gap by leveraging LLMs for enhanced product bundling. Bundle-LLM utilizes a hybrid item tokenization approach to integrate multimodal information, employing a multimodal fusion module and a trainable projector to embed non-textual features into a token, improving efficiency and synergy among modalities. Product bundling is formulated as a multiple-choice question with prompt templates. A progressive optimization strategy is employed to fine-tune LLMs for disentangled objectives, resulting in better semantic understanding and product bundling effectiveness. Extensive experiments show that Bundle-LLM outperforms state-of-the-art methods across multiple datasets and domains.

- **PhD-Level Questions**: [{'Question 1': 'Explain how Bundle-LLM integrates multimodal information and the role of the multimodal fusion module in this process. Why is this integration important for product bundling?', 'Answer 1': 'Bundle-LLM integrates multimodal information through a hybrid item tokenization approach where a multimodal fusion module is followed by a trainable projector. This fusion module embeds all non-textual features into a single token, thereby demonstrating the interplays among different modalities explicitly while also shortening the prompt length for better efficiency. This integration is crucial as it enables comprehensive semantic understanding and leveraging of multimodal data, which is essential for creating effective and strategic product bundles.'}, {'Question 2': 'What challenges do traditional Large Language Models (LLMs) face when applied directly to product bundling tasks, and how does Bundle-LLM address these challenges?', 'Answer 2': 'Traditional LLMs struggle with direct application to product bundling tasks due to their inability to process multimodal data effectively and exploit their knowledge for bundling purposes. Bundle-LLM addresses these challenges by introducing a hybrid item tokenization approach that allows multimodal data to be effectively embedded and combined. Furthermore, Bundle-LLM employs a prompt template to structure the bundling task as a multiple-choice question, and utilizes a progressive optimization strategy for fine-tuning LLMs, thereby improving their capability to handle comprehensive multimodal semantic understanding and creating more effective product bundles.'}]



### A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting (http://arxiv.org/abs/2407.11638v1) [pdf: http://arxiv.org/pdf/2407.11638v1]
- **Summary**: Large Language Models (LLMs) have shown significant potential in various data mining tasks, but their capacity for temporal event forecasting has not been thoroughly examined. This paper addresses this by evaluating LLM-based methods for forecasting temporal events. A new benchmark dataset, MidEast-TE-mini, is created for this purpose, combining graph and textual data. The study finds that simply integrating raw text into LLMs' inputs does not improve zero-shot extrapolation. However, incorporating text describing complex events and fine-tuning the models yields better performance. This is further improved with retrieval augmented generation (RAG) modules, which help capture temporal relational patterns in historical data. Despite these advancements, LLMs still face challenges such as popularity bias and the long-tail problem, especially in RAG-based methods. The evaluation provides insights and future research directions for enhancing temporal event forecasting with LLMs.

- **PhD-Level Questions**: [{'Question 1': 'Explain how the incorporation of complex event texts and fine-tuning improves the performance of LLMs in temporal event forecasting. Provide a detailed analysis based on the findings of the paper.'}, {'Question 2': 'Discuss the impacts of popularity bias and the long-tail problem on LLM-based temporal event forecasting. How do these issues particularly affect methods with retrieval augmented generation (RAG) modules?'}]



### Interactions with Generative Information Retrieval Systems (http://arxiv.org/abs/2407.11605v1) [pdf: http://arxiv.org/pdf/2407.11605v1]
Draft of a chapter intended to appear in a forthcoming book on
  generative information retrieval, co-edited by Chirag Shah and Ryen White

- **Summary**: The chapter delves into the interactive nature of information access and seeking, which is often constrained in traditional search engines to predefined actions. The authors highlight the potential of generative Information Retrieval (IR) systems to offer users a richer and more versatile way of expressing information needs and feedback through natural language and other modalities such as images, videos, gestures, and sensors. Different interaction aspects are discussed, including diverse ways users can communicate their information requirements, the provision of explicit or implicit feedback, and the various methods to refine retrieval results interactively. Additionally, the chapter explores mixed-initiative interactions, clarification, and preference elicitation, and how proactive generative IR systems can enhance user experience through context-aware recommendations, continuity in past and multi-party conversations, and feedback mechanisms. Other points of discussion include explanation as an interaction type and multi-modal interactions. The chapter concludes by describing emerging frameworks and solutions for user interfaces integrated with generative AI systems.

- **PhD-Level Questions**: [{'Question 1': "How do generative IR systems enhance a user's ability to express their information needs compared to traditional search engines? Provide examples incorporating multiple modalities."}, {'Question 2': 'Discuss the roles of explicit and implicit feedback in generative IR systems. How can these forms of feedback be utilized to improve information retrieval results?'}]



### A PLMs based protein retrieval framework (http://arxiv.org/abs/2407.11548v1) [pdf: http://arxiv.org/pdf/2407.11548v1]
16 pages, 12 figures

- **Summary**: The paper addresses the task of protein retrieval, which involves understanding the relationships between protein sequences, structures, and functions. The commonly used method, BLAST, focuses on sequence similarity, potentially missing out on homologous or functionally similar proteins that do not share high sequence identity. The authors propose a new framework leveraging Protein Language Models (PLMs) that represent protein sequences in a high-dimensional feature space. This enhanced representation allows for better retrieval of both similar and dissimilar proteins. An indexed vector database is constructed to expedite the retrieval process. Extensive experiments highlight the framework's ability to identify proteins that traditional methods might miss, thus supporting deeper biological insights and advancements.

- **PhD-Level Questions**: [{'Question 1': 'Explain how protein language models (PLMs) enhance the representation capacity of protein sequences compared to traditional sequence-similarity-based methods like BLAST.'}, {'Question 2': 'Discuss the advantages and potential limitations of using an accelerated indexed vector database for protein retrieval in the proposed framework.'}]



### Bootstrapped Pre-training with Dynamic Identifier Prediction for Generative Retrieval (http://arxiv.org/abs/2407.11504v1) [pdf: http://arxiv.org/pdf/2407.11504v1]
Accepted by ACL Findings 2024

- **Summary**: The paper introduces BootRet, a bootstrapped pre-training method for generative retrieval systems that addresses the limitation of static document identifiers by dynamically adjusting them throughout the pre-training process. This approach involves three key phases: (i) initial identifier generation, (ii) pre-training via corpus indexing and relevance prediction, and (iii) bootstrapping for updating identifiers. The method also incorporates noisy documents and pseudo-queries generated by large language models to improve the pre-training phase. Experimental results show that BootRet outperforms existing generative retrieval baselines and is effective even in zero-shot scenarios.

- **PhD-Level Questions**: [{'Question 1': 'How does BootRet address the problem of static document identifiers in generative retrieval, and what are the main phases involved in its methodology?'}, {'Question 2': 'Explain the role of noisy documents and pseudo-queries in the pre-training phase of BootRet. How do these components contribute to the overall effectiveness of the model?'}]



### EndoFinder: Online Image Retrieval for Explainable Colorectal Polyp Diagnosis (http://arxiv.org/abs/2407.11401v1) [pdf: http://arxiv.org/pdf/2407.11401v1]
MICCAI 2024

- **Summary**: The paper introduces EndoFinder, a content-based image retrieval framework designed to improve the explainability and efficiency of diagnosing malignant polyps during colonoscopy. Unlike traditional deep learning-based models that lack interpretability, EndoFinder uses a 'digital twin' approach where a new polyp is matched with similar polyps in a reference database to infer clinical semantics. It features a polyp-aware image encoder pre-trained on a large dataset through self-supervised learning, combining masked image modeling and contrastive learning into a generalized embedding space. The framework is validated through polyp re-identification and optical biopsy tasks, demonstrating performance comparable to supervised classification models while offering an explainable diagnostic process. EndoFinder supports real-time decision-making during colonoscopy by enabling image retrieval-based diagnostics.

- **PhD-Level Questions**: [{'Question 1': "Explain the concept of 'digital twin' as used in EndoFinder and discuss how it improves the explainability of polyp diagnosis during colonoscopy."}, {'Question 2': "Describe the role of the polyp-aware image encoder in EndoFinder. How does combining masked image modeling with contrastive learning contribute to the framework's performance?"}, {'Question 3': "Compare and contrast the efficacy of EndoFinder's content-based image retrieval approach with traditional supervised classification models in the context of optical biopsy and polyp re-identification.", 'Question 4': 'Discuss how self-supervised learning contributes to the development of EndoFinder. Why might this approach be particularly beneficial for medical image analysis tasks?'}, {'Question 5': 'Considering the potential for real-time application, what are some challenges and opportunities in integrating EndoFinder into actual colonoscopy procedures?'}]



### Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation (http://arxiv.org/abs/2407.11245v1) [pdf: http://arxiv.org/pdf/2407.11245v1]
Accepted at SIGIR'24

- **Summary**: The paper discusses improvements in Cross-Domain Sequential Recommendation (CDSR) by addressing the issue of negative transfer, which can occur when there is a lack of relation between domains or different levels of data sparsity. The proposed CDSR model dynamically estimates and assigns a weight factor to the prediction loss to control gradient flows through domains with significant negative transfer. The model evaluates the degree of negative transfer by comparing the performance of a multi-domain model (CDSR) with a single-domain model (SDSR). An auxiliary loss is also developed to maximize mutual information between SDSR and CDSR tasks on a per-domain basis, facilitating cooperative learning similar to the collaborative dynamics between pacers and runners in a marathon. The model demonstrated superior performance in experiments on real-world datasets across multiple service domains and showed a significant increase in click-through rate when deployed in a real-world recommendation system for a personal assistant app service.

- **PhD-Level Questions**: [{'Question 1': 'What are the primary challenges of applying Cross-Domain Sequential Recommendation (CDSR) compared to Single-Domain Sequential Recommendation (SDSR), and how does the proposed model address these challenges?'}, {'Question 2': "Explain the concept of 'negative transfer' in the context of cross-domain recommendations. How does the proposed model estimate and mitigate the effects of negative transfer between domains?"}, {'Question 3': 'Describe the role of the asymmetric cooperative network in the proposed model. How does it contribute to evaluating negative transfer and facilitating cooperative learning between SDSR and CDSR tasks?'}, {'Question 4': "Analyze the purpose and functioning of the auxiliary loss developed in the paper. How does maximizing mutual information between representation pairs from SDSR and CDSR tasks enhance the model's performance?"}, {'Question 5': "Discuss the real-world implications of the proposed CDSR model's implementation in a personal assistant app service. What measures of success were used, and what were the quantitative results?"}]



### BiasScanner: Automatic Detection and Classification of News Bias to Strengthen Democracy (http://arxiv.org/abs/2407.10829v1) [pdf: http://arxiv.org/pdf/2407.10829v1]
10 pages, 3 figures, 1 table

- **Summary**: BiasScanner is an innovative tool designed to enhance democratic engagement by aiding news readers in identifying biased content in online articles. The system uses a pre-trained large language model capable of detecting and categorizing over two dozen types of media bias at the sentence level. The tool is implemented as a server-side model paired with a front-end browser plugin, ensuring lightweight and privacy-respecting integration. In addition to detecting bias, the plugin provides detailed explanations for each biased sentence and a summary analysis of the overall article. BiasScanner stands out as the most fine-grained bias detection model currently in use and is the only deployed browser plugin of its kind.

- **PhD-Level Questions**: [{'Question 1': 'Explain the architecture and key components of BiasScanner. How does the combination of server-side pre-trained models and a browser plugin contribute to its functionality?'}, {'Question 2': 'BiasScanner is stated to detect more than two dozen types of media bias at the sentence level. Discuss the methodology and challenges associated with training a model to achieve such fine-grained discrimination. How does this compare to previous approaches?'}, {'Question 3': "Evaluate the implications of BiasScanner's lightweight and privacy-respecting design in the context of user acceptance and large-scale deployment. What are the potential trade-offs involved?"}, {'Question 4': 'Considering BiasScanner provides explanations for its decisions, how important is explainability in models dealing with media bias detection? Compare and contrast this with other applications of explainable AI.'}, {'Question 5': 'Critically analyze the potential societal impacts of deploying a tool like BiasScanner. What are the possible benefits and drawbacks, and how might such a tool influence public discourse and democracy?'}]



### SEMINAR: Search Enhanced Multi-modal Interest Network and Approximate Retrieval for Lifelong Sequential Recommendation (http://arxiv.org/abs/2407.10714v1) [pdf: http://arxiv.org/pdf/2407.10714v1]
9 pages,code released

- **Summary**: The paper addresses the challenges associated with modeling users' behaviors in recommendation systems, specifically focusing on the difficulty of training lifelong sequence models for Click Through Rate (CTR) prediction and Personalized Search Ranking (PSR). These difficulties arise due to insufficient learning of ID embeddings, and the struggle of existing target attention mechanisms to effectively handle multi-modal representations of items. To resolve this, the authors propose a unified lifelong multi-modal sequence model named SEMINAR, which stands for Search Enhanced Multi-Modal Interest Network and Approximate Retrieval. SEMINAR utilizes a network called Pretraining Search Unit (PSU) to learn multi-modal query-item pairs via a pretraining-finetuning method with multiple objectives, such as multi-modal alignment and query-item relevance prediction. The model also incorporates a codebook-based product quantization strategy for efficient online retrieval of multi-modal embeddings.

- **PhD-Level Questions**: [{'Question 1': 'Explain the insufficiency issues related to learning ID embeddings in lifelong sequence models for CTR and PSR predictions. How does the SEMINAR model propose to address these issues?'}, {'Question 2': 'Describe the multiple objectives that the Pretraining Search Unit (PSU) aims to achieve in the SEMINAR model. How do these objectives contribute to better modeling of lifelong multi-modal sequences?'}, {'Question 3': 'What is the role of multi-modal codebook-based product quantization in the SEMINAR model? Discuss its importance in accelerating the online retrieval speed of multi-modal embeddings.'}, {'Question 4': "Consider the multi-modal alignment challenge noted in the paper. Why is it crucial for effective recommendation systems, and how does SEMINAR's approach improve on existing target attention mechanisms?"}]



### $\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity (http://arxiv.org/abs/2407.10691v1) [pdf: http://arxiv.org/pdf/2407.10691v1]
- **Summary**: In the scientific domain, document retrieval systems have become increasingly important for generating responses by large language models (LLMs), particularly in scientific literature. However, existing dense retrievers face challenges with domain-specific retrieval and handling complex query-to-document relations, especially where queries relate to multiple document sections. To address these issues, the paper introduces a system called MixGR, which enhances dense retrievers by incorporating multiple levels of granularity in query-document matching through a zero-shot approach. MixGR combines different granular metrics into a single composite score that better represents query-document similarity. Experimental results indicate that MixGR significantly improves document retrieval performance, achieving a 24.7% increase in nDCG@5 compared to unsupervised methods and a 9.8% increase compared to supervised retrievers. Additionally, MixGR's effectiveness is demonstrated in downstream scientific question-answering tasks, underscoring its potential to enhance LLM applications in scientific contexts.

- **PhD-Level Questions**: [{'Question 1': 'What are the specific challenges associated with dense retrievers in domain-specific retrieval, and how does MixGR address these challenges?'}, {'Question 2': 'Explain the zero-shot approach utilized by MixGR and discuss its significance in improving query-document similarity metrics.'}, {'Question 3': 'How does MixGR achieve a balance between different granularities in query-document matching, and what metrics are fused to form the unified similarity score?'}, {'Question 4': 'Critically evaluate the performance metrics used in the paper, such as nDCG@5, and discuss their effectiveness in measuring the improvements brought by MixGR.'}, {'Question 5': 'Considering the downstream scientific question-answering tasks, how does the improvement in document retrieval by MixGR enhance the application of LLMs in scientific research?'}]



