New uploads on arXiv(cs.CL)

### Does Refusal Training in LLMs Generalize to the Past Tense? (http://arxiv.org/abs/2407.11969v1) [pdf: http://arxiv.org/pdf/2407.11969v1]
Code and jailbreak artifacts:
  https://github.com/tml-epfl/llm-past-tense

- **Summary**: The paper investigates a vulnerability in current refusal training methods for Large Language Models (LLMs), revealing that simply reformulating harmful queries in the past tense can effectively bypass these refusal mechanisms. The study systematically evaluates this loophole in various state-of-the-art LLMs, such as Llama-3 8B, GPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o, and R2D2, using GPT-3.5 Turbo to generate past tense reformulations. The attack significantly increases the success rate of harmful outputs, especially notable in GPT-4o where the success rate rose from 1% to 88% with repeated attempts. Future tense reformulations, in contrast, were less effective. The paper also shows that fine-tuning models to include past tense examples can mitigate this vulnerability. These findings suggest that existing alignment techniques like SFT, RLHF, and adversarial training might be inadequate in certain contexts. The authors provide relevant code and artifacts to support their findings.

- **PhD-Level Questions**: [{'Question 1': 'Discuss the implications of the identified generalization gap in refusal training for the safety and ethical considerations of deploying LLMs in real-world applications.'}, {'Question 2': 'Considering the findings on the effectiveness of past vs. future tense reformulations, how might these insights inform improvements in current model refusal training methodologies?'}, {'Question 3': "Explain the potential reasons for the observed discrepancy in refusal guardrails' effectiveness between past and future tense reformulations of harmful queries."}, {'Question 4': "Analyze how the study's use of GPT-3.5 Turbo as a reformulation model and GPT-4 as a jailbreak judge affects the validity and generalizability of the results."}, {'Question 5': 'Evaluate the feasibility and potential limitations of fine-tuning models with past tense examples as a solution to the identified generalization gap in refusal training.'}]



### NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window? (http://arxiv.org/abs/2407.11963v1) [pdf: http://arxiv.org/pdf/2407.11963v1]
- **Summary**: The paper introduces NeedleBench, a framework designed to evaluate the long-context capabilities of large language models (LLMs). This involves identifying relevant content from long documents to answer user queries. NeedleBench presents progressively more challenging tasks across various length intervals (ranging from 4k to 1000k and beyond) and depths within the text, assessing models' ability to retrieve and reason with key information in bilingual long texts. The framework includes the Ancestral Trace Challenge (ATC) to simulate complex logical reasoning scenarios found in real-world applications. The study reveals that current LLMs struggle with practical long-context applications, especially in logical reasoning challenges.

- **PhD-Level Questions**: [{'Question 1': 'Explain the significance of the NeedleBench framework in evaluating the capabilities of large language models. How does it differentiate from previous evaluation methods?'}, {'Question 2': 'Discuss the challenges posed by the Ancestral Trace Challenge (ATC) within NeedleBench and how it contributes to understanding the limitations of current LLMs in long-context scenarios.'}]



### Rethinking Transformer-based Multi-document Summarization: An Empirical Investigation (http://arxiv.org/abs/2407.11948v1) [pdf: http://arxiv.org/pdf/2407.11948v1]
- **Summary**: This paper investigates the performance and behavior of Transformer-based models in multi-document summarization (MDS). Through five empirical studies, the paper examines the impact of document boundary separators, the effectiveness of different Transformer structures, the sensitivity of the encoder and decoder, different training strategies, and the problem of repetition in generated summaries. Experimental results on widely-used MDS datasets and eleven evaluation metrics show that document boundary separators, the granularity of different level features, and various training strategies significantly influence MDS. Furthermore, the results indicate that the decoder is more sensitive to noise than the encoder, underlining its critical role. The study also finds that repetition in summaries is correlated with high uncertainty scores, providing insights for future research directions in improving summarization models.

- **PhD-Level Questions**: [{'Question 1': 'Discuss the significance of document boundary separators in Transformer-based MDS models as presented in the paper. How do the empirical studies demonstrate their impact on summary quality?'}, {'Question 2': 'The paper notes that the decoder exhibits greater sensitivity to noise compared to the encoder. Based on the findings, propose potential modifications or research directions that could address this sensitivity issue in Transformer-based MDS models.'}, {'Question 3': 'Analyze the correlation between repetition problems in generated summaries and high uncertainty scores as discussed in the paper. What mechanisms might underlie this correlation, and how could it inform future improvements in MDS models?'}, {'Question 4': 'Evaluate the effectiveness of different mainstream Transformer structures in the context of MDS as explored in the paper. What are the key differences between the structures studied, and how do they influence the summarization performance?'}, {'Question 5': 'Discuss the role of different training strategies on the performance of Transformer-based MDS models. What insights does the paper provide on this topic, and how might these insights be leveraged to enhance model training and summarization results?'}]



### Fine-grained Hallucination Detection and Mitigation in Long-form Question Answering (http://arxiv.org/abs/2407.11930v1) [pdf: http://arxiv.org/pdf/2407.11930v1]
Code and data are available:
  https://github.com/UKPLab/arxiv2024-lfqa-hallucination

- **Summary**: Long-form question answering (LFQA) aims to produce detailed responses to complex questions but faces challenges like hallucinations and factual inconsistencies. To address this, the authors introduce HaluQuestQA, a dataset that includes 698 QA pairs with 4.7k span-level error annotations for various error types. Using this data, they analyze gaps in comprehensiveness and reference unhelpfulness in LFQA. They also train an automatic feedback model to predict error spans and provide explanations. A new prompt-based approach called Error-informed refinement utilizes feedback from the model to enhance generated answers, showing reduced hallucinations and improved quality. Human evaluators also prefer responses generated by this method over baseline answers.

- **PhD-Level Questions**: [{'Question 1': 'What are the five different error types annotated in the HaluQuestQA dataset, and how do they impact the quality of LFQA answers?'}, {'Question 2': 'Describe the methodology used to train the automatic feedback model for predicting error spans and providing explanations in LFQA. How does this model improve the quality of the generated answers?'}, {'Question 3': 'Explain the proposed Error-informed refinement approach. How does it utilize signals from the feedback model to reduce hallucinations and improve LFQA answer quality?'}, {'Question 4': 'Analyze the significance of using 698 QA pairs and 4.7k span-level error annotations. How does this dataset size and granularity contribute to the robustness and reliability of the study’s findings?'}, {'Question 5': 'Critically evaluate the preference judgments made by human evaluators in the study. What subjective and objective factors might influence the 84% preference for answers generated by the proposed approach over baseline answers?'}]



### What's Wrong? Refining Meeting Summaries with LLM Feedback (http://arxiv.org/abs/2407.11919v1) [pdf: http://arxiv.org/pdf/2407.11919v1]
- **Summary**: Meeting summarization has grown in importance with the rise of digital meetings. While Large Language Models (LLMs) have advanced in summarization tasks by improving coherence and context understanding, they still face issues like maintaining relevance and avoiding hallucinations. This paper introduces a novel multi-LLM correction approach to enhance meeting summaries through a two-phase process: mistake identification and summary refinement. The authors have released the QMSum Mistake dataset, consisting of 200 meeting summaries annotated by humans for nine types of errors such as structural, omission, and irrelevance errors. The study demonstrates that LLMs can accurately identify these errors, and the transformation of identified mistakes into actionable feedback enhances summary quality. This multi-LLM approach shows promise for complex text generation tasks that demand robustness and precise action planning.

- **PhD-Level Questions**: [{'Question 1': 'Explain the two-phase process of mistake identification and summary refinement proposed by the authors for improving meeting summarization. How do these phases interact to enhance the quality of summaries produced by LLMs?'}, {'Question 2': 'Describe the error types annotated in the QMSum Mistake dataset. How do these specific types of errors impact the quality of an automatically generated meeting summary?'}, {'Question 3': 'Discuss the significance of actionable feedback in the context of summary refinement. How does transforming identified mistakes into actionable feedback contribute to the robustness and quality of meeting summaries?'}, {'Question 4': 'Evaluate how the multi-LLM approach can be generalized to other complex text generation tasks. What are the potential benefits and challenges of applying this approach to different domains?'}, {'Question 5': 'The paper claims that LLMs can identify errors with high accuracy. What methods or techniques are used to train LLMs for mistake identification, and what metrics are used to measure their accuracy?'}]



### A Novel Lexicon for the Moral Foundation of Liberty (http://arxiv.org/abs/2407.11862v1) [pdf: http://arxiv.org/pdf/2407.11862v1]
- **Summary**: The paper revolves around the importance of the moral value of liberty in forming opinions on controversial social issues like vaccine hesitancy, climate change, and abortion rights. It introduces a new Liberty lexicon, evaluated on over 3,000 manually annotated data points across various domains. The final lexicon combines information from multiple lexicons generated using word embedding similarity (WE) and compositional semantics (CS). Key contributions include the enhancement of liberty annotations, development of a robust liberty lexicon for broader applications, and revealing the complexity of liberty-related expressions across different platforms. The evaluation demonstrates that the task's difficulty necessitates approaches that combine diverse knowledge to improve learning system representations.

- **PhD-Level Questions**: [{'Question 1': 'Explain how word embedding similarity (WE) and compositional semantics (CS) contribute to the generation of the Liberty lexicon. How do these methods complement each other in this context?'}, {'Question 2': 'Discuss the implications of incorporating a Liberty lexicon into automated systems addressing social issues. What are some potential benefits and challenges associated with this integration?'}, {'Question 3': 'The paper mentions the complexity of liberty-related expressions across different platforms. Provide an analysis of why platform-specific variations might arise and how they can be addressed in the development of generalized lexical resources.'}, {'Question 4': 'In what ways could the final combined lexicon enhance the performance of machine learning models in understanding and processing social discourse? Provide theoretical and practical examples.'}]



### Evaluating Task-Oriented Dialogue Consistency through Constraint Satisfaction (http://arxiv.org/abs/2407.11857v1) [pdf: http://arxiv.org/pdf/2407.11857v1]
- **Summary**: The paper focuses on maintaining consistency in task-oriented dialogues by conceptualizing dialogue consistency as a Constraint Satisfaction Problem (CSP). In this framework, variables represent segments of the dialogue that reference the conversational domain, while constraints reflect linguistic, conversational, and domain-specific properties. The authors employ a CSP solver to detect inconsistencies in dialogues that have been re-lexicalized by a Large Language Model (LLM). Their findings show that CSP effectively detects inconsistencies, but generating consistent dialogue remains challenging for state-of-the-art LLMs, which achieved only 0.15 accuracy when compared to the CSP solver. An ablation study indicates that domain knowledge constraints are the hardest to respect. The paper argues that CSP better captures essential properties of dialogue consistency than current component-based approaches.

- **PhD-Level Questions**: [{'Question 1': 'Explain how the Constraint Satisfaction Problem (CSP) framework is applied to maintain logical coherence in task-oriented dialogues. What are the variables and constraints in this context?'}, {'Question 2': 'The paper states that state-of-the-art LLMs achieve only a 0.15 accuracy rate when re-lexicalizing dialogues compared to a CSP solver. Why might this be the case, and what implications does it have for the development of future dialogue systems?'}, {'Question 3': 'Discuss the significance of constraints derived from domain knowledge in the context of CSP-based dialogue consistency. Why do these constraints pose the greatest difficulty, according to the ablation study?'}, {'Question 4': 'Compare and contrast the CSP-based approach to dialogue consistency with component pipeline-based approaches. What are the core properties of dialogue consistency that the former captures more effectively?'}]



### Scaling Sign Language Translation (http://arxiv.org/abs/2407.11855v1) [pdf: http://arxiv.org/pdf/2407.11855v1]
- **Summary**: The paper focuses on advancing sign language translation (SLT) by scaling pretraining data, model size, and the number of translation directions. It addresses the limitations of existing studies that often cover narrow domains and few sign languages, struggling with open-domain tasks. The authors perform large-scale SLT pretraining using various data sources: noisy multilingual YouTube SLT data, parallel text corpora, and SLT data augmented through machine translation of video captions. They employ task-specific prompts within an encoder-decoder architecture and initialize the SLT model with pretrained (m/By)T5 models of various sizes. Results on How2Sign and FLEURS-ASL#0 (translating ASL to 42 spoken languages) highlight the importance of scaling data/models and cross-lingual cross-modal transfer. The study also demonstrates the feasibility of zero-shot SLT. Finetuning the pretrained SLT models on five open-domain benchmarks for five different sign languages shows significant improvements over baseline models, achieving state-of-the-art results.

- **PhD-Level Questions**: [{'Question 1': 'Explain the significance of using task-specific prompts in the context of the encoder-decoder architecture for large-scale SLT pretraining. How do these prompts contribute to model performance?', 'Answer 1': "Task-specific prompts help the model differentiate between the various tasks it was pretrained on, ensuring that the encoder-decoder architecture correctly interprets the input and generates the appropriate output for each task. These prompts improve model performance by guiding the model’s attention mechanisms and parameter utilization more efficiently, which is particularly important when dealing with multilingual and multi-modality data. The prompts help in aligning the model's structure with the specific requirements of SLT, leading to better generalization and transfer learning outcomes."}, {'Question 2': 'Discuss the impact of scaling data and model sizes in improving the performance of SLT models. What are the potential challenges and benefits associated with this scaling?', 'Answer 2': "Scaling data and model sizes significantly enhances the model's ability to capture diverse linguistic patterns and nuances, thus improving translation quality. Larger datasets provide more varied examples, fostering better generalization, while larger models can learn more complex features and relationships within the data. Challenges include increased computational resources and training time, the need for more sophisticated methods to ensure data quality, and potential overfitting. Benefits involve improved model robustness, higher accuracy across multiple languages, and better handling of open-domain tasks, as demonstrated by the substantial performance gains over previous state-of-the-art benchmarks."}]



### Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection (http://arxiv.org/abs/2407.11854v1) [pdf: http://arxiv.org/pdf/2407.11854v1]
Submitted to EMNLP 2024

- **Summary**: The paper tackles the issue of Grammatical Error Detection (GED) in low-resource languages where human-annotated error corpora are unavailable. Leveraging the zero-shot cross-lingual transfer capabilities of multilingual pre-trained language models, the researchers propose generating synthetic error corpora by using data from a diverse set of languages. They introduce a two-stage fine-tuning pipeline where the GED model is first fine-tuned on multilingual synthetic data from target languages and then on human-annotated GED corpora from source languages. This method surpasses current annotation-free GED methods and produces more diverse, human-like errors compared to existing approaches.

- **PhD-Level Questions**: [{'Question 1': 'How does the two-stage fine-tuning pipeline proposed in the paper enhance the performance of GED models in low-resource languages compared to single-stage fine-tuning methods?'}, {'Question 2': 'Explain the mechanism by which zero-shot cross-lingual transfer capabilities of multilingual pre-trained language models are utilized to generate synthetic errors in the target low-resource languages.'}, {'Question 3': 'Discuss the evaluation metrics used to compare the proposed GED method with existing state-of-the-art annotation-free GED methods. What makes the errors generated by the proposed method more human-like?'}, {'Question 4': 'In the context of the paper, what are the advantages and potential limitations of using synthetic error corpora for training GED models in low-resource languages?'}, {'Question 5': 'Consider the diversity in the set of source languages used for generating synthetic errors. How does this diversity contribute to the effectiveness of the two-stage fine-tuning approach described in the paper?'}]



### InferAct: Inferring Safe Actions for LLM-Based Agents Through Preemptive Evaluation and Human Feedback (http://arxiv.org/abs/2407.11843v1) [pdf: http://arxiv.org/pdf/2407.11843v1]
- **Summary**: The paper introduces InferAct, an approach designed to enhance the decision-making robustness of LLM-based agents in real-life applications, particularly where preventing risky or irreversible errors is critical. The current research landscape lacks methods for preemptively evaluating the reasoning trajectories of these agents, which InferAct aims to address by leveraging the Theory-of-Mind capability intrinsic to LLMs. This capability allows for the proactive detection of potential errors before crucial actions are taken. InferAct can also integrate human feedback to mitigate risks further and refine the decision-making process of the agents. The approach is validated through experiments on three commonly used tasks, demonstrating its effectiveness for secure and reliable agent deployment in environments requiring critical decision-making.

- **PhD-Level Questions**: [{'Question 1': 'How does InferAct leverage Theory-of-Mind capabilities within LLMs to detect potential errors in decision-making processes before critical actions are executed?'}, {'Question 2': 'What are the key experiments conducted to validate the effectiveness of InferAct, and what metrics were used to measure its success in enhancing the robustness and reliability of LLM-based agents?'}]



New uploads on arXiv(cs.IR)

### Harnessing Large Language Models for Multimodal Product Bundling (http://arxiv.org/abs/2407.11712v2) [pdf: http://arxiv.org/pdf/2407.11712v2]
under review

- **Summary**: The paper introduces Bundle-LLM, a novel approach to product bundling that leverages large language models (LLMs). Traditional bundling methods have limitations in semantic understanding, scope of knowledge, and handling cold-start issues. While LLMs have robust knowledge and reasoning capabilities, their direct application in processing multimodal information is inadequate. Bundle-LLM addresses these challenges by employing a hybrid item tokenization technique that integrates multimodal information. This method utilizes a fusion module followed by a trainable projector to embed non-textual features into a single token, enhancing efficiency and multimodal interaction. A prompt template is used to frame product bundling as a multiple-choice question, and a progressive optimization strategy fine-tunes the LLMs for disentangled objectives. Experiments on datasets from different application domains show that Bundle-LLM outperforms existing state-of-the-art methods.

- **PhD-Level Questions**: [{'Question 1': 'Explain the hybrid item tokenization strategy used in Bundle-LLM and its role in integrating multimodal information. How does this method enhance the interplays among different modalities and improve efficiency?', 'Answer': 'The hybrid item tokenization strategy in Bundle-LLM integrates multimodal information by embedding non-textual features into a single token through a multimodal fusion module followed by a trainable projector. This method leverages both textual and non-textual data, making the interactions among different modalities explicit. By reducing the prompt length, the strategy enhances processing efficiency and ensures comprehensive semantic understanding across modalities.'}, {'Question 2': 'Discuss the significance of progressive optimization strategy in fine-tuning LLMs for product bundling tasks. What challenges does this strategy address, and how does it contribute to achieving effective product bundling capability?', 'Answer': 'The progressive optimization strategy fine-tunes LLMs by addressing disentangled objectives step-by-step. This approach tackles the complexity of multimodal interactions and the extensive knowledge required for effective bundling. By progressively optimizing the models, it ensures that the LLMs develop a nuanced understanding of different modalities, leading to better performance in product bundling tasks. This strategy is particularly crucial for achieving comprehensive semantic understanding and handling diverse information sources effectively.'}]



### A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting (http://arxiv.org/abs/2407.11638v1) [pdf: http://arxiv.org/pdf/2407.11638v1]
- **Summary**: The paper investigates the capabilities of Large Language Models (LLMs) in the domain of temporal event forecasting. The study begins with the creation of a high-quality dataset named MidEast-TE-mini, designed to include both graph and textual data. Using this dataset, the authors evaluate various baseline methods which incorporate different input formats and Retrieval-Augmented Generation (RAG) modules. Key findings reveal that simply integrating raw texts into LLMs does not improve zero-shot extrapolation performance. However, fine-tuning the LLMs with raw texts on specific complex events and augmenting them with retrieval modules significantly enhances their performance. Persistent issues like popularity bias and the long-tail problem are also noted, especially in methods based on RAG. The research highlights promising directions for future studies in LLM-based temporal event forecasting.

- **PhD-Level Questions**: [{'Question 1': 'Explain the significance of the MidEast-TE-mini dataset in the context of temporal event forecasting and how it addresses limitations in existing datasets.'}, {'Question 2': 'Analyze the impact of fine-tuning Large Language Models (LLMs) with raw texts on specific complex events versus using raw texts in a zero-shot setting for temporal event forecasting.'}]



### Interactions with Generative Information Retrieval Systems (http://arxiv.org/abs/2407.11605v1) [pdf: http://arxiv.org/pdf/2407.11605v1]
Draft of a chapter intended to appear in a forthcoming book on
  generative information retrieval, co-edited by Chirag Shah and Ryen White

- **Summary**: The chapter focuses on the interactive nature of information retrieval (IR) systems, specifically the transition towards generative IR systems that allow richer and more versatile interactions. Traditional search engines limit user actions to basic interactions like querying, clicking, and scrolling. In contrast, generative IR systems facilitate free-form interactions through natural language and other modalities, such as images, videos, and gestures. The chapter delves into how users can express information needs, provide feedback, and refine retrieval results interactively. It also covers advanced topics like mixed-initiative interactions, context-aware recommendations, multi-party conversations, and the provision of explanations. Multi-modal interactions and emerging frameworks for user interfaces with generative AI systems are also discussed.

- **PhD-Level Questions**: [{'Question 1': 'Discuss the advantages and potential limitations of multi-modal generative IR systems compared to traditional search engines in terms of user interaction.'}, {'Question 2': 'Explain the concept of mixed-initiative interactions in generative IR systems and provide examples of how this can enhance the user experience.'}, {'Question 3': 'How can generative IR systems utilize user feedback, both explicit and implicit, to refine retrieval results? Provide examples illustrating this process.'}, {'Question 4': 'In the context of context-aware recommendation systems within generative IR, how does the system leverage past conversations to improve the relevance of its suggestions?'}, {'Question 5': 'What are the challenges and solutions associated with implementing multi-party conversations in generative IR systems?'}, {'Question 6': 'Evaluate the role of explanation in generative IR systems. How can providing explanations improve user trust and system transparency?'}, {'Question 7': 'Identify the emerging frameworks and solutions for user interfaces with generative AI systems described in the chapter. How do these frameworks address the challenges posed by generative interactions?'}]



### A PLMs based protein retrieval framework (http://arxiv.org/abs/2407.11548v1) [pdf: http://arxiv.org/pdf/2407.11548v1]
16 pages, 12 figures

- **Summary**: This paper addresses the limitations of traditional protein retrieval methods, such as BLAST, which prioritize sequence similarity and may overlook proteins that are dissimilar in sequence but share homology or functionality. The authors propose a novel protein retrieval framework that leverages protein language models (PLMs) to embed protein sequences in a high-dimensional feature space, enhancing representation capacity for subsequent analysis. An accelerated indexed vector database is then constructed to facilitate quick access and retrieval of these dense vectors. Experiments demonstrate that the proposed framework can effectively retrieve both similar and dissimilar proteins, identifying proteins that conventional methods may miss, thus supporting protein mining and biological research advancement.

- **PhD-Level Questions**: [{'Question 1': 'Explain the limitations of sequence similarity-based methods such as BLAST in protein retrieval and how the proposed framework addresses these limitations. Discuss the role of protein language models (PLMs) in this context.'}, {'Question 2': 'Describe the process of embedding protein sequences into a high-dimensional feature space using protein language models (PLMs). How does this embedding enhance the representation capacity for subsequent analysis?'}, {'Question 3': 'What are the advantages of using an accelerated indexed vector database in the proposed framework? How does it facilitate the expedited access and retrieval of dense vectors?'}, {'Question 4': 'Design an experiment to test the efficacy of the proposed protein retrieval framework compared to traditional methods. What metrics would you use to evaluate performance?'}, {'Question 5': 'Critically analyze the ability of the proposed framework to identify proteins that conventional methods fail to uncover. How might this impact the field of protein mining and biological research?'}]



### Bootstrapped Pre-training with Dynamic Identifier Prediction for Generative Retrieval (http://arxiv.org/abs/2407.11504v1) [pdf: http://arxiv.org/pdf/2407.11504v1]
Accepted by ACL Findings 2024

- **Summary**: The paper discusses generative retrieval, which uses differentiable search indexes to generate document identifiers in response to a query. Traditional generative retrieval models face limitations due to their reliance on static document identifiers that may not adapt to evolving model parameters. The authors introduce BootRet, a bootstrapped pre-training method for generative retrieval, which dynamically updates document identifiers during pre-training. BootRet consists of three key phases: initial identifier generation, pre-training with corpus indexing and relevance prediction tasks, and bootstrapping for updating identifiers. Additionally, the pre-training phase involves the use of noisy documents and pseudo-queries generated by large language models to mirror semantic connections in the tasks. Experimental results show that BootRet outperforms existing generative retrieval pre-training baselines and is effective in zero-shot settings.

- **PhD-Level Questions**: [{'Question 1': 'Explain the importance of dynamic document identifiers in the context of generative retrieval and how BootRet addresses this need.'}, {'Question 2': 'Describe the three key phases of BootRet and discuss how each phase contributes to improving pre-training for generative retrieval models.'}, {'Question 3': 'How do noisy documents and pseudo-queries generated by large language models facilitate the pre-training process in BootRet, and what advantages do they offer over traditional pre-training methods?'}, {'Question 4': "Critically analyze the experimental results provided for BootRet. What benchmarks were used, and how does BootRet's performance compare to existing baselines in generative retrieval?"}, {'Question 5': "Discuss the implications of BootRet's effectiveness in zero-shot settings and how this characteristic could impact real-world applications of generative retrieval models."}]



### EndoFinder: Online Image Retrieval for Explainable Colorectal Polyp Diagnosis (http://arxiv.org/abs/2407.11401v1) [pdf: http://arxiv.org/pdf/2407.11401v1]
MICCAI 2024

- **Summary**: The paper addresses the challenge of determining the necessity of resecting malignant polyps during colonoscopy. Traditional histopathology examination is time-consuming and costly, and while deep learning models have shown promise with endoscopic images, they often lack explainability. The authors introduce EndoFinder, a content-based image retrieval framework designed to identify a 'digital twin' polyp from a reference database when a new polyp is detected. This approach utilizes a polyp-aware image encoder pre-trained on a large dataset using self-supervised techniques that combine masked image modeling and contrastive learning. The resulting embedding space is versatile for various downstream clinical tasks. The framework is validated through experiments on polyp re-identification and optical biopsy tasks, demonstrating that EndoFinder provides explainable diagnostics while matching the performance of supervised classification models. The image retrieval mechanism of EndoFinder supports real-time decision-making during colonoscopy procedures.

- **PhD-Level Questions**: [{'Question 1': 'Discuss the advantages and potential limitations of using a content-based image retrieval framework like EndoFinder in clinical settings compared to traditional deep learning classification models.'}, {'Question 2': 'Explain the role of self-supervised learning and how it was implemented in the EndoFinder framework for pre-training the polyp-aware image encoder.'}, {'Question 3': 'How does the combination of masked image modeling and contrastive learning contribute to the robustness and effectiveness of EndoFinder’s embedding space?'}, {'Question 4': 'Evaluate the implications of EndoFinder achieving performance comparable to supervised models, particularly in the context of explainable diagnostics and clinical adoption.'}, {'Question 5': 'Design an experiment to test the generalizability of EndoFinder’s embedding space to other types of endoscopic images beyond polyp identification.'}, {'Question 6': 'How would you address the potential challenge of varying image quality and different endoscopic equipment when deploying EndoFinder in diverse clinical environments?'}]



### Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation (http://arxiv.org/abs/2407.11245v1) [pdf: http://arxiv.org/pdf/2407.11245v1]
Accepted at SIGIR'24

- **Summary**: The paper discusses a Cross-Domain Sequential Recommendation (CDSR) system that improves recommendation performance by leveraging information from multiple domains compared to Single-Domain Sequential Recommendation (SDSR). A key challenge addressed is negative transfer, which occurs when domains are weakly related or have differing data sparsity levels, potentially degrading performance. The proposed model mitigates negative transfer by estimating its degree in each domain and adjusting the weight in the prediction loss function accordingly. The model utilizes an asymmetric cooperative network to compare CDSR and SDSR performance and employs an auxiliary loss function to enhance mutual information between their representations. This cooperative learning approach is likened to the dynamic between pacers and runners in a marathon. The model outperforms previous systems in experiments using two real-world datasets across ten service domains, and in production, it achieved a 21.4% increase in click-through rate for a personal assistant app's recommendation system.

- **PhD-Level Questions**: [{'Question 1': 'Explain the concept of negative transfer in the context of Cross-Domain Sequential Recommendation (CDSR) and describe how the proposed model addresses this issue.'}, {'Question 2': 'Discuss the role of the asymmetric cooperative network in evaluating negative transfer. How does it help in determining the appropriate weight for the prediction loss function?'}, {'Question 3': 'What is the significance of maximizing mutual information between representation pairs from SDSR and CDSR tasks? How does this strategy contribute to the overall performance of the recommendation system?'}, {'Question 4': 'Compare and contrast the proposed CDSR model with traditional SDSR approaches. What are the main benefits and potential drawbacks of the cooperative learning mechanism employed in the CDSR model?'}, {'Question 5': 'How did the authors validate the performance improvements of their proposed model? Discuss the methodology and the significance of the results obtained from real-world industrial datasets.'}, {'Question 6': 'The paper mentions the implementation of the model in a personal assistant app service leading to significant business value. Discuss the implications of deploying such advanced recommendation systems in real-world applications and the potential impact on user engagement.'}]



### BiasScanner: Automatic Detection and Classification of News Bias to Strengthen Democracy (http://arxiv.org/abs/2407.10829v1) [pdf: http://arxiv.org/pdf/2407.10829v1]
10 pages, 3 figures, 1 table

- **Summary**: The paper introduces BiasScanner, an application that aims to combat issues like disinformation, biased reporting, and hate speech in online news consumption. BiasScanner uses a server-side pre-trained large language model to identify and classify over two dozen types of media bias at the sentence level. It is implemented as a web browser plug-in, making it the first of its kind to offer such fine-grained bias detection and classification in a deployed, user-friendly manner. Besides identifying biased sentences, the application also provides explanations for its decisions and summary analyses for entire articles. The paper emphasizes the importance of such a tool in strengthening democracy by helping news consumers critically assess the articles they read.

- **PhD-Level Questions**: [{'Question 1': 'Explain the significance of utilizing a server-side pre-trained large language model for BiasScanner and how it differs from earlier methods in bias detection.'}, {'Question 2': 'Discuss the potential ethical implications and privacy concerns associated with the deployment of BiasScanner, and how the developers addressed these concerns.'}, {'Question 3': 'BiasScanner claims to identify and classify over two dozen types of media bias. Elaborate on the challenges and methodologies involved in training a model to achieve this fine-grained classification.'}, {'Question 4': 'Compare and contrast the impact of BiasScanner with that of traditional fact-checking and bias-detection approaches in terms of real-time usability and accuracy.'}]



### SEMINAR: Search Enhanced Multi-modal Interest Network and Approximate Retrieval for Lifelong Sequential Recommendation (http://arxiv.org/abs/2407.10714v1) [pdf: http://arxiv.org/pdf/2407.10714v1]
9 pages,code released

- **Summary**: The paper introduces SEMINAR (Search Enhanced Multi-Modal Interest Network and Approximate Retrieval), a unified model designed to address the challenges in user behavior modeling for recommendation systems, particularly focusing on lifelong sequences. Traditional methods face difficulties due to issues like insufficient learning of ID embeddings and misaligned multi-modal item representations. SEMINAR proposes a Pretraining Search Unit (PSU) to handle lifelong sequences of multi-modal query-item pairs through a pretraining-finetuning approach. This model incorporates multiple objectives, including multi-modal alignment and predictive tasks, and further speeds up retrieval via a codebook-based product quantization strategy.

- **PhD-Level Questions**: [{'Question 1': 'Explain the challenges faced by traditional user behavior modeling techniques in recommendation systems when dealing with lifelong sequences. How does SEMINAR aim to overcome these challenges?'}, {'Question 2': "Describe the role of the Pretraining Search Unit (PSU) in SEMINAR. What are the main objectives during the pretraining phase, and how do these contribute to the model's performance during the finetuning phase?"}, {'Question 3': 'What is the significance of multi-modal alignment in the context of SEMINAR, and what methods does the model employ to achieve this alignment?'}, {'Question 4': 'Discuss the multi-modal codebook-based product quantization strategy proposed in SEMINAR. How does this strategy improve the online retrieval speed, and why is this improvement significant?'}]



### $\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity (http://arxiv.org/abs/2407.10691v1) [pdf: http://arxiv.org/pdf/2407.10691v1]
- **Summary**: This paper introduces $	exttt{MixGR}$, a novel approach for improving dense retrievers' performance in scientific document retrieval. Dense retrievers often face challenges with domain-specific retrieval and complex query-document relationships. $	exttt{MixGR}$ addresses these issues by enhancing query-document matching across varying granularities using a zero-shot approach. It merges different metrics to produce a unified score that better represents query-document similarity. The approach demonstrated significant improvements in performance, particularly in scientific datasets, and its benefits extend to enhancing the performance of downstream scientific question-answering tasks.

- **PhD-Level Questions**: [{'Question 1': 'What are the main challenges faced by dense retrievers in domain-specific retrieval and how does $\texttt{MixGR}$ propose to overcome them?'}, {'Question 2': 'Discuss the zero-shot approach utilized by $\texttt{MixGR}$. How does incorporating various granularities improve query-document similarity scoring?'}, {'Question 3': 'Explain the experimental setup and metrics used to evaluate the performance of $\texttt{MixGR}$. What improvements were observed and how statistically significant are these results?'}, {'Question 4': 'How does $\texttt{MixGR}$ compare with traditional unsupervised and supervised retrievers on scientific retrieval datasets in terms of nDCG@5? Provide a detailed analysis of the results.'}, {'Question 5': 'Evaluate the impact of $\texttt{MixGR}$ on scientific question-answering tasks. How does it contribute to the effectiveness of LLMs in these tasks?'}]



